{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4\n",
    "\n",
    "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Finally, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the *Penn Treebank* dataset.\n",
    "\n",
    "As always, we will start by copying this notebook and getting the starting code.\n",
    "Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "%cd /content/drive/MyDrive/10714\n",
    "!git clone https://github.com/dlsys10714/hw4.git\n",
    "%cd /content/drive/MyDrive/10714/hw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the datasets you will be using for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10-batches-py/\n",
      "cifar-10-batches-py/data_batch_4\n",
      "cifar-10-batches-py/readme.html\n",
      "cifar-10-batches-py/test_batch\n",
      "cifar-10-batches-py/data_batch_3\n",
      "cifar-10-batches-py/batches.meta\n",
      "cifar-10-batches-py/data_batch_2\n",
      "cifar-10-batches-py/data_batch_5\n",
      "cifar-10-batches-py/data_batch_1\n"
     ]
    }
   ],
   "source": [
    "!python3 ./apps/download_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish setting up the assignment, go ahead and fill in all the code between `### BEGIN YOUR SOLUTION` and `### END YOUR SOLUTION` using your solution code from previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 9.4.0\n",
      "-- The CXX compiler identification is GNU 9.4.0\n",
      "-- Check for working C compiler: /usr/bin/cc\n",
      "-- Check for working C compiler: /usr/bin/cc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++\n",
      "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Python: /home/willyu/anaconda3/envs/10-414/bin/python3.9 (found version \"3.9.13\") found components: Development Interpreter \n",
      "-- Performing Test HAS_FLTO\n",
      "-- Performing Test HAS_FLTO - Success\n",
      "-- Found pybind11: /home/willyu/anaconda3/envs/10-414/lib/python3.9/site-packages/pybind11/include (found version \"2.10.0\")\n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
      "-- Looking for pthread_create in pthreads\n",
      "-- Looking for pthread_create in pthreads - not found\n",
      "-- Looking for pthread_create in pthread\n",
      "-- Looking for pthread_create in pthread - found\n",
      "-- Found Threads: TRUE  \n",
      "-- Found CUDA: /home/willyu/anaconda3/envs/10-414 (found version \"11.7\") \n",
      "-- Found cuda, building cuda backend\n",
      "Sat Dec  3 14:07:02 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:06:00.0 Off |                  N/A |\n",
      "| 44%   85C    P2   291W / 350W |  21379MiB / 24268MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |                  N/A |\n",
      "|100%   81C    P2   150W / 350W |  22334MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |                  N/A |\n",
      "| 85%   68C    P2   166W / 350W |  21792MiB / 24268MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1410      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    214751      C   ...onda3/envs/utv/bin/python    21363MiB |\n",
      "|    1   N/A  N/A      1410      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A    216040      C   ...onda3/envs/utv/bin/python    14817MiB |\n",
      "|    1   N/A  N/A   1016133      C   ...onda3/envs/utv/bin/python     7497MiB |\n",
      "|    2   N/A  N/A      1410      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A    217306      C   ...onda3/envs/utv/bin/python    14275MiB |\n",
      "|    2   N/A  N/A   1016134      C   ...onda3/envs/utv/bin/python     7497MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.6 8.6 8.6\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/willyu/10-414/hw4/build\n",
      "make[1]: Entering directory '/home/willyu/10-414/hw4/build'\n",
      "make[2]: Entering directory '/home/willyu/10-414/hw4/build'\n",
      "make[3]: Entering directory '/home/willyu/10-414/hw4/build'\n",
      "[ 25%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target ndarray_backend_cuda\u001b[0m\n",
      "make[3]: Leaving directory '/home/willyu/10-414/hw4/build'\n",
      "make[3]: Entering directory '/home/willyu/10-414/hw4/build'\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cuda.cpython-39-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/home/willyu/10-414/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cuda\n",
      "make[3]: Entering directory '/home/willyu/10-414/hw4/build'\n",
      "\u001b[35m\u001b[1mScanning dependencies of target ndarray_backend_cpu\u001b[0m\n",
      "make[3]: Leaving directory '/home/willyu/10-414/hw4/build'\n",
      "make[3]: Entering directory '/home/willyu/10-414/hw4/build'\n",
      "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cpu.cpython-39-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/home/willyu/10-414/hw4/build'\n",
      "[100%] Built target ndarray_backend_cpu\n",
      "make[2]: Leaving directory '/home/willyu/10-414/hw4/build'\n",
      "make[1]: Leaving directory '/home/willyu/10-414/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ND Backend [10 pts]\n",
    "\n",
    "Fill in the following classes in `python/needle/ops.py`:\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `Transpose`\n",
    "- `Reshape`\n",
    "- `BroadcastTo`<br>\n",
    "  Call `compact()` before NDArray `broadcast_to()`, see [here](https://forum.dlsyscourse.org/t/test-ndarray-backend-with-test-optim-py-hw2/2821/3).\n",
    "- `Summation`<br>\n",
    "  `summation()` is called in the backward pass of `BroadcastTo`. Thus, its backend should support multi-axes reduction, which is not true in Homework 3. The call stack of `summation()` forward pass is\n",
    "  |     |     |\n",
    "  | --- | --- |\n",
    "  | `ops.py` | `summation` |\n",
    "  | `autograd.py` | `TensorOp.__call__` |\n",
    "  | `autograd.py` | `Tensor.make_from_op` |\n",
    "  | `autograd.py` |`Value.realize_cached_data` |\n",
    "  | `ops.py` | `Summation.compute`|\n",
    "  | `backend_ndarray/ndarray.py` | `summation` |\n",
    "  | `backend_ndarray/ndarray.py` | `NDArray.sum` |\n",
    "  | `backend_ndarray/ndarray_backend_*.*` | `reduce_sum` |\n",
    "- `MatMul`\n",
    "- `Negate`\n",
    "- `Log`\n",
    "- `Exp`\n",
    "- `ReLU`\n",
    "- `LogSumExp`\n",
    "- `Tanh` (new)<br>\n",
    "  $$\n",
    "  \\frac{d \\tanh}{d x} = 1 - (\\tanh x)^2\n",
    "  $$\n",
    "- `Stack` (new)\n",
    "- `Split` (new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for most of these, you already wrote the solutions in the previous homework, and you do not need to change your previous solution. However, `Tanh`, `Stack`, and `Split` are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written **in terms of each other**. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
    "\n",
    "**Note:** You may want to make `Summation` support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
    "\n",
    "**Note:** Depending on your implementations, you may want to ensure that you **call `compact()` before reshaping arrays**. (If this is necessary, you will run into corresponding error messages later in the assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.13, pytest-7.1.3, pluggy-1.0.0 -- /home/willyu/anaconda3/envs/10-414/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/willyu/10-414/hw4\n",
      "plugins: anyio-3.6.1\n",
      "collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  0%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  1%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  2%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  3%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  4%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  6%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m       [  7%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m     [  8%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m       [  9%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 10%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m    [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 12%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m    [ 13%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 14%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 15%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 17%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 18%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 19%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 20%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 21%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 23%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 24%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 25%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 26%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 28%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 29%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 30%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 31%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 32%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 34%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 35%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                    [ 36%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                    [ 37%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                    [ 39%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                    [ 40%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 41%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 42%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 43%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 45%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 46%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 47%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 48%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 49%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 51%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 52%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 53%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 54%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 56%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 57%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 58%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 59%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 60%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 62%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 63%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 64%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 65%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 67%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 68%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 69%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 71%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 74%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 75%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 76%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m      [ 80%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m      [ 81%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 82%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 84%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 85%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 86%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 87%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 89%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 90%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 91%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 92%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 93%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 95%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 96%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 97%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 98%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 99%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m           [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________ test_scalar_fn[cpu-shape0-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f223a1143a0>, shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.4221624]]])\n",
      "_A         = array([[[-1.4221624]]], dtype=float32)\n",
      "_B         = -0.019735336303710938\n",
      "device     = cpu()\n",
      "fn         = <function <lambda> at 0x7f223a1143a0>\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\n",
      "        a          = needle.Tensor([[[-1.4221624]]])\n",
      "        b          = -0.019735336303710938\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:307: in __truediv__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\n",
      "        other      = -0.019735336303710938\n",
      "        self       = needle.Tensor([[[-1.4221624]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.4221624]]]),)\n",
      "        self       = <needle.ops.DivScalar object at 0x7f2239b17730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.4221624]]]),)\n",
      "        op         = <needle.ops.DivScalar object at 0x7f2239b17730>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f2239a66130>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f2239a66130>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.DivScalar object at 0x7f2239b17730>\n",
      "a = NDArray([[[-1.4221624]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# WARNING\u001b[39;49;00m\n",
      "        \u001b[90m# Dividing a float by an interger may introduce dtype mismatch. Fellows\u001b[39;49;00m\n",
      "        \u001b[90m# on the forums reports `float32 / int` yields `float64`, although I\u001b[39;49;00m\n",
      "        \u001b[90m# did not encounter this issue.\u001b[39;49;00m\n",
      "        \u001b[90m#\u001b[39;49;00m\n",
      "        \u001b[90m# Type alignment is pivotal in that optimizers shall not assign weights\u001b[39;49;00m\n",
      "        \u001b[90m# of different type than the original one.\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.divide(a, \u001b[96mself\u001b[39;49;00m.scalar,\n",
      "                                dtype=a.dtype)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.4221624]]], device=cpu())\n",
      "self       = <needle.ops.DivScalar object at 0x7f2239b17730>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:187: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_scalar_fn[cpu-shape1-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f223a1143a0>, shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022....44244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]])\n",
      "_A         = array([[[-0.15259542, -0.03079642,  1.289313  ,  0.5304533 ,\n",
      "         -0.74591106, -0.28878465],\n",
      "        [ 1.204689  ,...64 ],\n",
      "        [-1.2044502 ,  0.9504057 , -0.3747067 , -0.3207342 ,\n",
      "         -0.98206496, -1.7369602 ]]], dtype=float32)\n",
      "_B         = -1.2508771419525146\n",
      "device     = cpu()\n",
      "fn         = <function <lambda> at 0x7f223a1143a0>\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\n",
      "        a          = needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022....44244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]])\n",
      "        b          = -1.2508771419525146\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:307: in __truediv__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\n",
      "        other      = -1.2508771419525146\n",
      "        self       = needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022....44244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.580302...4244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]]),)\n",
      "        self       = <needle.ops.DivScalar object at 0x7f2231e4e6a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.580302...4244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]]),)\n",
      "        op         = <needle.ops.DivScalar object at 0x7f2231e4e6a0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f2231e4eb20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f2231e4eb20>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.DivScalar object at 0x7f2231e4e6a0>\n",
      "a = NDArray([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022  -0.3...5791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# WARNING\u001b[39;49;00m\n",
      "        \u001b[90m# Dividing a float by an interger may introduce dtype mismatch. Fellows\u001b[39;49;00m\n",
      "        \u001b[90m# on the forums reports `float32 / int` yields `float64`, although I\u001b[39;49;00m\n",
      "        \u001b[90m# did not encounter this issue.\u001b[39;49;00m\n",
      "        \u001b[90m#\u001b[39;49;00m\n",
      "        \u001b[90m# Type alignment is pivotal in that optimizers shall not assign weights\u001b[39;49;00m\n",
      "        \u001b[90m# of different type than the original one.\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.divide(a, \u001b[96mself\u001b[39;49;00m.scalar,\n",
      "                                dtype=a.dtype)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022  -0.3...5791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]], device=cpu())\n",
      "self       = <needle.ops.DivScalar object at 0x7f2231e4e6a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:187: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_scalar_fn[cuda-shape0-divide] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f223a1143a0>, shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.21264285]]])\n",
      "_A         = array([[[-0.21264285]]], dtype=float32)\n",
      "_B         = -0.6414637565612793\n",
      "device     = cuda()\n",
      "fn         = <function <lambda> at 0x7f223a1143a0>\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\n",
      "        a          = needle.Tensor([[[-0.21264285]]])\n",
      "        b          = -0.6414637565612793\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:307: in __truediv__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\n",
      "        other      = -0.6414637565612793\n",
      "        self       = needle.Tensor([[[-0.21264285]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.21264285]]]),)\n",
      "        self       = <needle.ops.DivScalar object at 0x7f2231e6ca90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.21264285]]]),)\n",
      "        op         = <needle.ops.DivScalar object at 0x7f2231e6ca90>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f2231e6c880>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f2231e6c880>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.DivScalar object at 0x7f2231e6ca90>\n",
      "a = NDArray([[[-0.21264285]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# WARNING\u001b[39;49;00m\n",
      "        \u001b[90m# Dividing a float by an interger may introduce dtype mismatch. Fellows\u001b[39;49;00m\n",
      "        \u001b[90m# on the forums reports `float32 / int` yields `float64`, although I\u001b[39;49;00m\n",
      "        \u001b[90m# did not encounter this issue.\u001b[39;49;00m\n",
      "        \u001b[90m#\u001b[39;49;00m\n",
      "        \u001b[90m# Type alignment is pivotal in that optimizers shall not assign weights\u001b[39;49;00m\n",
      "        \u001b[90m# of different type than the original one.\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.divide(a, \u001b[96mself\u001b[39;49;00m.scalar,\n",
      "                                dtype=a.dtype)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.21264285]]], device=cuda())\n",
      "self       = <needle.ops.DivScalar object at 0x7f2231e6ca90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:187: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_scalar_fn[cuda-shape1-divide] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f223a1143a0>, shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.7119254...0.0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]])\n",
      "_A         = array([[[ 0.17852722, -0.48561108,  0.4178631 ,  0.6601184 ,\n",
      "         -0.26862556,  0.04397374],\n",
      "        [ 0.11434969,...873],\n",
      "        [ 0.95702404,  0.3420432 , -1.1888571 , -1.3259709 ,\n",
      "         -1.4491868 ,  0.3708898 ]]], dtype=float32)\n",
      "_B         = -0.7234068512916565\n",
      "device     = cuda()\n",
      "fn         = <function <lambda> at 0x7f223a1143a0>\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\n",
      "        a          = needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.7119254...0.0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]])\n",
      "        b          = -0.7234068512916565\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:307: in __truediv__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\n",
      "        other      = -0.7234068512916565\n",
      "        self       = needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.7119254...0.0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.711925...0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]]),)\n",
      "        self       = <needle.ops.DivScalar object at 0x7f2231e34700>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.711925...0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]]),)\n",
      "        op         = <needle.ops.DivScalar object at 0x7f2231e34700>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f2231e349d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f2231e349d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.DivScalar object at 0x7f2231e34700>\n",
      "a = NDArray([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.71192545 -0.3...6092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# WARNING\u001b[39;49;00m\n",
      "        \u001b[90m# Dividing a float by an interger may introduce dtype mismatch. Fellows\u001b[39;49;00m\n",
      "        \u001b[90m# on the forums reports `float32 / int` yields `float64`, although I\u001b[39;49;00m\n",
      "        \u001b[90m# did not encounter this issue.\u001b[39;49;00m\n",
      "        \u001b[90m#\u001b[39;49;00m\n",
      "        \u001b[90m# Type alignment is pivotal in that optimizers shall not assign weights\u001b[39;49;00m\n",
      "        \u001b[90m# of different type than the original one.\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.divide(a, \u001b[96mself\u001b[39;49;00m.scalar,\n",
      "                                dtype=a.dtype)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.71192545 -0.3...6092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]], device=cuda())\n",
      "self       = <needle.ops.DivScalar object at 0x7f2231e34700>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:187: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-16-16-16] ___________________________\u001b[0m\n",
      "\n",
      "m = 16, n = 16, p = 16, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.51...959696 -1.3108143  -0.5683854   0.5310244  -0.60321134 -0.50895303\n",
      "   0.3437534   0.05532323 -0.36882105  0.01935508]])\n",
      "B          = needle.Tensor([[-0.92069906  0.8202185  -1.1294544  -1.6430022   0.385828   -1.3562282\n",
      "  -0.9987398  -0.26050347  0.66...65777  -0.00881964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]])\n",
      "_A         = array([[-0.15554655, -1.2261544 , -1.3876351 ,  0.3273014 ,  0.45741168,\n",
      "         0.8751152 ,  0.31749257, -0.4368339 ...310244 ,\n",
      "        -0.60321134, -0.50895303,  0.3437534 ,  0.05532323, -0.36882105,\n",
      "         0.01935508]], dtype=float32)\n",
      "_B         = array([[-0.92069906,  0.8202185 , -1.1294544 , -1.6430022 ,  0.385828  ,\n",
      "        -1.3562282 , -0.9987398 , -0.26050347...1863153,\n",
      "        -0.75460845,  0.06351615, -1.9021302 , -1.2813807 , -0.5920339 ,\n",
      "        -0.49041998]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 16\n",
      "n          = 16\n",
      "p          = 16\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-0.92069906  0.8202185  -1.1294544  -1.6430022   0.385828   -1.3562282\n",
      "  -0.9987398  -0.26050347  0.66...65777  -0.00881964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]])\n",
      "        self       = needle.Tensor([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.51...959696 -1.3108143  -0.5683854   0.5310244  -0.60321134 -0.50895303\n",
      "   0.3437534   0.05532323 -0.36882105  0.01935508]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.5...5777  -0.00881964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231de31f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.5...5777  -0.00881964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231de31f0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231de3880>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231de3880>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231de31f0>\n",
      "a = NDArray([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.51824284...143  -0.5683854   0.5310244  -0.60321134 -0.50895303\n",
      "   0.3437534   0.05532323 -0.36882105  0.01935508]], device=cpu())\n",
      "b = NDArray([[-0.92069906  0.8202185  -1.1294544  -1.6430022   0.385828   -1.3562282\n",
      "  -0.9987398  -0.26050347  0.6609252 ...1964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.51824284...143  -0.5683854   0.5310244  -0.60321134 -0.50895303\n",
      "   0.3437534   0.05532323 -0.36882105  0.01935508]], device=cpu())\n",
      "b          = NDArray([[-0.92069906  0.8202185  -1.1294544  -1.6430022   0.385828   -1.3562282\n",
      "  -0.9987398  -0.26050347  0.6609252 ...1964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231de31f0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_matmul[cpu-8-8-8] ____________________________\u001b[0m\n",
      "\n",
      "m = 8, n = 8, p = 8, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-1...3101  -0.24976124]\n",
      " [ 0.14081745  0.5460881  -0.39847532 -1.4903992  -0.36902663  0.529036\n",
      "  -0.86960596 -1.1056118 ]])\n",
      "B          = needle.Tensor([[-1.0530033  -0.28650165 -0.17814207 -0.11176766 -1.3285084  -0.02897426\n",
      "  -0.6904589  -1.0077598 ]\n",
      " [-...957   1.2171819 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]])\n",
      "_A         = array([[ 0.48591456,  1.177145  , -1.466795  ,  1.6390375 , -0.7933805 ,\n",
      "        -0.4654586 ,  1.0839599 , -2.443315  ...45,  0.5460881 , -0.39847532, -1.4903992 , -0.36902663,\n",
      "         0.529036  , -0.86960596, -1.1056118 ]], dtype=float32)\n",
      "_B         = array([[-1.0530033 , -0.28650165, -0.17814207, -0.11176766, -1.3285084 ,\n",
      "        -0.02897426, -0.6904589 , -1.0077598 ...4 , -0.03515908,  1.3605766 , -0.35897616, -0.72593737,\n",
      "        -1.1327294 , -1.3635881 ,  0.8229569 ]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 8\n",
      "n          = 8\n",
      "p          = 8\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-1.0530033  -0.28650165 -0.17814207 -0.11176766 -1.3285084  -0.02897426\n",
      "  -0.6904589  -1.0077598 ]\n",
      " [-...957   1.2171819 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]])\n",
      "        self       = needle.Tensor([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-1...3101  -0.24976124]\n",
      " [ 0.14081745  0.5460881  -0.39847532 -1.4903992  -0.36902663  0.529036\n",
      "  -0.86960596 -1.1056118 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-...57   1.2171819 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e56250>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-...57   1.2171819 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e56250>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e56af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e56af0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e56250>\n",
      "a = NDArray([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-1.49407...124]\n",
      " [ 0.14081745  0.5460881  -0.39847532 -1.4903992  -0.36902663  0.529036\n",
      "  -0.86960596 -1.1056118 ]], device=cpu())\n",
      "b = NDArray([[-1.0530033  -0.28650165 -0.17814207 -0.11176766 -1.3285084  -0.02897426\n",
      "  -0.6904589  -1.0077598 ]\n",
      " [-1.9156...9 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-1.49407...124]\n",
      " [ 0.14081745  0.5460881  -0.39847532 -1.4903992  -0.36902663  0.529036\n",
      "  -0.86960596 -1.1056118 ]], device=cpu())\n",
      "b          = NDArray([[-1.0530033  -0.28650165 -0.17814207 -0.11176766 -1.3285084  -0.02897426\n",
      "  -0.6904589  -1.0077598 ]\n",
      " [-1.9156...9 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e56250>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_matmul[cpu-1-2-3] ____________________________\u001b[0m\n",
      "\n",
      "m = 1, n = 2, p = 3, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-0.06555045 -1.8084983 ]])\n",
      "B          = needle.Tensor([[ 1.5185491  1.2726929  1.1502954]\n",
      " [-1.8556256  0.0994349  1.29319  ]])\n",
      "_A         = array([[-0.06555045, -1.8084983 ]], dtype=float32)\n",
      "_B         = array([[ 1.5185491,  1.2726929,  1.1502954],\n",
      "       [-1.8556256,  0.0994349,  1.29319  ]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 1\n",
      "n          = 2\n",
      "p          = 3\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 1.5185491  1.2726929  1.1502954]\n",
      " [-1.8556256  0.0994349  1.29319  ]])\n",
      "        self       = needle.Tensor([[-0.06555045 -1.8084983 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.06555045 -1.8084983 ]]), needle.Tensor([[ 1.5185491  1.2726929  1.1502954]\n",
      " [-1.8556256  0.0994349  1.29319  ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e6a6a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.06555045 -1.8084983 ]]), needle.Tensor([[ 1.5185491  1.2726929  1.1502954]\n",
      " [-1.8556256  0.0994349  1.29319  ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e6a6a0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e6abb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e6abb0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e6a6a0>\n",
      "a = NDArray([[-0.06555045 -1.8084983 ]], device=cpu())\n",
      "b = NDArray([[ 1.5185491  1.2726929  1.1502954]\n",
      " [-1.8556256  0.0994349  1.29319  ]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-0.06555045 -1.8084983 ]], device=cpu())\n",
      "b          = NDArray([[ 1.5185491  1.2726929  1.1502954]\n",
      " [-1.8556256  0.0994349  1.29319  ]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e6a6a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_matmul[cpu-3-4-5] ____________________________\u001b[0m\n",
      "\n",
      "m = 3, n = 4, p = 5, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-1.0421938  -0.8071687  -0.24530506 -2.0273237 ]\n",
      " [ 0.21127886  0.98698556  0.6603719   1.4917945 ]\n",
      " [ 0.09001131 -0.03438774 -0.192898   -0.70543116]])\n",
      "B          = needle.Tensor([[-1.0797551  -0.7484756  -0.08977896 -0.5458425   0.10092839]\n",
      " [ 0.07315691  0.6343583  -0.5220256  -2....027    0.7476647  -0.41365138 -0.20997673 -0.09096657]\n",
      " [-0.47489336  0.19243455  0.6839716   1.2178586  -0.36567354]])\n",
      "_A         = array([[-1.0421938 , -0.8071687 , -0.24530506, -2.0273237 ],\n",
      "       [ 0.21127886,  0.98698556,  0.6603719 ,  1.4917945 ],\n",
      "       [ 0.09001131, -0.03438774, -0.192898  , -0.70543116]],\n",
      "      dtype=float32)\n",
      "_B         = array([[-1.0797551 , -0.7484756 , -0.08977896, -0.5458425 ,  0.10092839],\n",
      "       [ 0.07315691,  0.6343583 , -0.5220256...20997673, -0.09096657],\n",
      "       [-0.47489336,  0.19243455,  0.6839716 ,  1.2178586 , -0.36567354]],\n",
      "      dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 3\n",
      "n          = 4\n",
      "p          = 5\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-1.0797551  -0.7484756  -0.08977896 -0.5458425   0.10092839]\n",
      " [ 0.07315691  0.6343583  -0.5220256  -2....027    0.7476647  -0.41365138 -0.20997673 -0.09096657]\n",
      " [-0.47489336  0.19243455  0.6839716   1.2178586  -0.36567354]])\n",
      "        self       = needle.Tensor([[-1.0421938  -0.8071687  -0.24530506 -2.0273237 ]\n",
      " [ 0.21127886  0.98698556  0.6603719   1.4917945 ]\n",
      " [ 0.09001131 -0.03438774 -0.192898   -0.70543116]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-1.0421938  -0.8071687  -0.24530506 -2.0273237 ]\n",
      " [ 0.21127886  0.98698556  0.6603719   1.4917945 ]\n",
      " ...27    0.7476647  -0.41365138 -0.20997673 -0.09096657]\n",
      " [-0.47489336  0.19243455  0.6839716   1.2178586  -0.36567354]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e44e80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-1.0421938  -0.8071687  -0.24530506 -2.0273237 ]\n",
      " [ 0.21127886  0.98698556  0.6603719   1.4917945 ]\n",
      " ...27    0.7476647  -0.41365138 -0.20997673 -0.09096657]\n",
      " [-0.47489336  0.19243455  0.6839716   1.2178586  -0.36567354]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e44e80>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e44f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e44f10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e44e80>\n",
      "a = NDArray([[-1.0421938  -0.8071687  -0.24530506 -2.0273237 ]\n",
      " [ 0.21127886  0.98698556  0.6603719   1.4917945 ]\n",
      " [ 0.09001131 -0.03438774 -0.192898   -0.70543116]], device=cpu())\n",
      "b = NDArray([[-1.0797551  -0.7484756  -0.08977896 -0.5458425   0.10092839]\n",
      " [ 0.07315691  0.6343583  -0.5220256  -2.078690...47  -0.41365138 -0.20997673 -0.09096657]\n",
      " [-0.47489336  0.19243455  0.6839716   1.2178586  -0.36567354]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-1.0421938  -0.8071687  -0.24530506 -2.0273237 ]\n",
      " [ 0.21127886  0.98698556  0.6603719   1.4917945 ]\n",
      " [ 0.09001131 -0.03438774 -0.192898   -0.70543116]], device=cpu())\n",
      "b          = NDArray([[-1.0797551  -0.7484756  -0.08977896 -0.5458425   0.10092839]\n",
      " [ 0.07315691  0.6343583  -0.5220256  -2.078690...47  -0.41365138 -0.20997673 -0.09096657]\n",
      " [-0.47489336  0.19243455  0.6839716   1.2178586  -0.36567354]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e44e80>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_matmul[cpu-5-4-3] ____________________________\u001b[0m\n",
      "\n",
      "m = 5, n = 4, p = 3, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-0.2937384  -0.3344175   0.02704735  1.0617678 ]\n",
      " [-0.75799483  2.2069716   0.5547965   1.111104  ]\n",
      " [...25  0.04709983]\n",
      " [-2.0375242   0.49982682 -1.681943    0.38202986]\n",
      " [ 1.0276988  -0.0483133  -0.15898733 -0.15437916]])\n",
      "B          = needle.Tensor([[-0.20431733  0.2830978   0.9333507 ]\n",
      " [ 0.675912    1.5356629   0.13157348]\n",
      " [-0.4686602  -0.41999525 -1.1867639 ]\n",
      " [-0.33185792 -0.5316489   1.8490043 ]])\n",
      "_A         = array([[-0.2937384 , -0.3344175 ,  0.02704735,  1.0617678 ],\n",
      "       [-0.75799483,  2.2069716 ,  0.5547965 ,  1.111104 ...49982682, -1.681943  ,  0.38202986],\n",
      "       [ 1.0276988 , -0.0483133 , -0.15898733, -0.15437916]],\n",
      "      dtype=float32)\n",
      "_B         = array([[-0.20431733,  0.2830978 ,  0.9333507 ],\n",
      "       [ 0.675912  ,  1.5356629 ,  0.13157348],\n",
      "       [-0.4686602 , -0.41999525, -1.1867639 ],\n",
      "       [-0.33185792, -0.5316489 ,  1.8490043 ]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 5\n",
      "n          = 4\n",
      "p          = 3\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-0.20431733  0.2830978   0.9333507 ]\n",
      " [ 0.675912    1.5356629   0.13157348]\n",
      " [-0.4686602  -0.41999525 -1.1867639 ]\n",
      " [-0.33185792 -0.5316489   1.8490043 ]])\n",
      "        self       = needle.Tensor([[-0.2937384  -0.3344175   0.02704735  1.0617678 ]\n",
      " [-0.75799483  2.2069716   0.5547965   1.111104  ]\n",
      " [...25  0.04709983]\n",
      " [-2.0375242   0.49982682 -1.681943    0.38202986]\n",
      " [ 1.0276988  -0.0483133  -0.15898733 -0.15437916]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.2937384  -0.3344175   0.02704735  1.0617678 ]\n",
      " [-0.75799483  2.2069716   0.5547965   1.111104  ]\n",
      " ... [ 0.675912    1.5356629   0.13157348]\n",
      " [-0.4686602  -0.41999525 -1.1867639 ]\n",
      " [-0.33185792 -0.5316489   1.8490043 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231dd29a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.2937384  -0.3344175   0.02704735  1.0617678 ]\n",
      " [-0.75799483  2.2069716   0.5547965   1.111104  ]\n",
      " ... [ 0.675912    1.5356629   0.13157348]\n",
      " [-0.4686602  -0.41999525 -1.1867639 ]\n",
      " [-0.33185792 -0.5316489   1.8490043 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231dd29a0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dd2d30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dd2d30>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231dd29a0>\n",
      "a = NDArray([[-0.2937384  -0.3344175   0.02704735  1.0617678 ]\n",
      " [-0.75799483  2.2069716   0.5547965   1.111104  ]\n",
      " [-0.903...]\n",
      " [-2.0375242   0.49982682 -1.681943    0.38202986]\n",
      " [ 1.0276988  -0.0483133  -0.15898733 -0.15437916]], device=cpu())\n",
      "b = NDArray([[-0.20431733  0.2830978   0.9333507 ]\n",
      " [ 0.675912    1.5356629   0.13157348]\n",
      " [-0.4686602  -0.41999525 -1.1867639 ]\n",
      " [-0.33185792 -0.5316489   1.8490043 ]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-0.2937384  -0.3344175   0.02704735  1.0617678 ]\n",
      " [-0.75799483  2.2069716   0.5547965   1.111104  ]\n",
      " [-0.903...]\n",
      " [-2.0375242   0.49982682 -1.681943    0.38202986]\n",
      " [ 1.0276988  -0.0483133  -0.15898733 -0.15437916]], device=cpu())\n",
      "b          = NDArray([[-0.20431733  0.2830978   0.9333507 ]\n",
      " [ 0.675912    1.5356629   0.13157348]\n",
      " [-0.4686602  -0.41999525 -1.1867639 ]\n",
      " [-0.33185792 -0.5316489   1.8490043 ]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231dd29a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-16-16-32] ___________________________\u001b[0m\n",
      "\n",
      "m = 16, n = 16, p = 32, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0.1...16476  -0.3986269  -0.43782985  0.28514323  0.11035689 -0.14837153\n",
      "  -1.6391121   0.83434004  0.69749874 -0.38745925]])\n",
      "B          = needle.Tensor([[-1.14337194e+00  2.49391809e-01  1.08130765e+00 -1.70937479e+00\n",
      "  -1.10590553e+00  7.21969530e-02 -8.0...00  5.66456497e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]])\n",
      "_A         = array([[ 0.59100014,  1.8669811 , -1.8348835 ,  0.37579638,  1.1054361 ,\n",
      "         0.40346032, -0.37935227,  0.25318664...8514323,\n",
      "         0.11035689, -0.14837153, -1.6391121 ,  0.83434004,  0.69749874,\n",
      "        -0.38745925]], dtype=float32)\n",
      "_B         = array([[-1.14337194e+00,  2.49391809e-01,  1.08130765e+00,\n",
      "        -1.70937479e+00, -1.10590553e+00,  7.21969530e-02,\n",
      "...,\n",
      "        -4.79118615e-01,  6.65913820e-01, -1.02438211e+00,\n",
      "        -7.88112700e-01,  1.99212804e-01]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 16\n",
      "n          = 16\n",
      "p          = 32\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-1.14337194e+00  2.49391809e-01  1.08130765e+00 -1.70937479e+00\n",
      "  -1.10590553e+00  7.21969530e-02 -8.0...00  5.66456497e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]])\n",
      "        self       = needle.Tensor([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0.1...16476  -0.3986269  -0.43782985  0.28514323  0.11035689 -0.14837153\n",
      "  -1.6391121   0.83434004  0.69749874 -0.38745925]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0....0  5.66456497e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231ddedc0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0....0  5.66456497e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231ddedc0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dde880>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dde880>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231ddedc0>\n",
      "a = NDArray([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0.1862415...269  -0.43782985  0.28514323  0.11035689 -0.14837153\n",
      "  -1.6391121   0.83434004  0.69749874 -0.38745925]], device=cpu())\n",
      "b = NDArray([[-1.14337194e+00  2.49391809e-01  1.08130765e+00 -1.70937479e+00\n",
      "  -1.10590553e+00  7.21969530e-02 -8.0922144...e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0.1862415...269  -0.43782985  0.28514323  0.11035689 -0.14837153\n",
      "  -1.6391121   0.83434004  0.69749874 -0.38745925]], device=cpu())\n",
      "b          = NDArray([[-1.14337194e+00  2.49391809e-01  1.08130765e+00 -1.70937479e+00\n",
      "  -1.10590553e+00  7.21969530e-02 -8.0922144...e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231ddedc0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-64-64-64] ___________________________\u001b[0m\n",
      "\n",
      "m = 64, n = 64, p = 64, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.9119931....09954476 -0.16885027\n",
      "  -0.2995055 ]\n",
      " [-2.1042256  -0.13245133 -0.45385894 ... -2.5528316   0.04279845\n",
      "  -0.29738396]])\n",
      "B          = needle.Tensor([[ 1.5264846e-01 -1.4349681e+00  4.3614903e-03 ...  1.2838923e+00\n",
      "   7.0336229e-01  1.1196541e-01]\n",
      " [ 2....03 -1.9393919e-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]])\n",
      "_A         = array([[-0.28366467,  1.2510917 , -0.01465345, ..., -0.708219  ,\n",
      "        -1.5997269 , -1.0858743 ],\n",
      "       [-1.1571369... ],\n",
      "       [-2.1042256 , -0.13245133, -0.45385894, ..., -2.5528316 ,\n",
      "         0.04279845, -0.29738396]], dtype=float32)\n",
      "_B         = array([[ 1.5264846e-01, -1.4349681e+00,  4.3614903e-03, ...,\n",
      "         1.2838923e+00,  7.0336229e-01,  1.1196541e-01],\n",
      "...4082e+00, -3.1304705e-01,  1.2746793e+00, ...,\n",
      "         6.5945369e-01,  4.2147583e-01,  1.5114847e+00]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 64\n",
      "n          = 64\n",
      "p          = 64\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 1.5264846e-01 -1.4349681e+00  4.3614903e-03 ...  1.2838923e+00\n",
      "   7.0336229e-01  1.1196541e-01]\n",
      " [ 2....03 -1.9393919e-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]])\n",
      "        self       = needle.Tensor([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.9119931....09954476 -0.16885027\n",
      "  -0.2995055 ]\n",
      " [-2.1042256  -0.13245133 -0.45385894 ... -2.5528316   0.04279845\n",
      "  -0.29738396]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.911993...3 -1.9393919e-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e3e580>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.911993...3 -1.9393919e-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e3e580>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e3e7c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e3e7c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e3e580>\n",
      "a = NDArray([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.9119931  -2.5...6885027\n",
      "  -0.2995055 ]\n",
      " [-2.1042256  -0.13245133 -0.45385894 ... -2.5528316   0.04279845\n",
      "  -0.29738396]], device=cpu())\n",
      "b = NDArray([[ 1.5264846e-01 -1.4349681e+00  4.3614903e-03 ...  1.2838923e+00\n",
      "   7.0336229e-01  1.1196541e-01]\n",
      " [ 2.045734...-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.9119931  -2.5...6885027\n",
      "  -0.2995055 ]\n",
      " [-2.1042256  -0.13245133 -0.45385894 ... -2.5528316   0.04279845\n",
      "  -0.29738396]], device=cpu())\n",
      "b          = NDArray([[ 1.5264846e-01 -1.4349681e+00  4.3614903e-03 ...  1.2838923e+00\n",
      "   7.0336229e-01  1.1196541e-01]\n",
      " [ 2.045734...-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e3e580>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-72-72-72] ___________________________\u001b[0m\n",
      "\n",
      "m = 72, n = 72, p = 72, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.038116...0.59203815  0.15285783\n",
      "  -1.1825513 ]\n",
      " [-0.84942585  0.3735221  -0.31763205 ... -1.2876897  -1.1643226\n",
      "  -0.42393202]])\n",
      "B          = needle.Tensor([[-0.34177306  0.80441475  1.7493241  ...  2.3223765  -0.22549094\n",
      "   0.87994814]\n",
      " [ 0.62408024  0.171024...-2.3511746  -3.2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]])\n",
      "_A         = array([[ 0.4375507 , -0.8506252 ,  0.63876194, ..., -0.25451216,\n",
      "        -0.31289756,  0.04666944],\n",
      "       [-0.4258043... ],\n",
      "       [-0.84942585,  0.3735221 , -0.31763205, ..., -1.2876897 ,\n",
      "        -1.1643226 , -0.42393202]], dtype=float32)\n",
      "_B         = array([[-0.34177306,  0.80441475,  1.7493241 , ...,  2.3223765 ,\n",
      "        -0.22549094,  0.87994814],\n",
      "       [ 0.6240802... ],\n",
      "       [-0.16189983,  0.03018835, -1.2238582 , ...,  0.33606574,\n",
      "        -0.6597182 ,  3.3332303 ]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 72\n",
      "n          = 72\n",
      "p          = 72\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-0.34177306  0.80441475  1.7493241  ...  2.3223765  -0.22549094\n",
      "   0.87994814]\n",
      " [ 0.62408024  0.171024...-2.3511746  -3.2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]])\n",
      "        self       = needle.Tensor([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.038116...0.59203815  0.15285783\n",
      "  -1.1825513 ]\n",
      " [-0.84942585  0.3735221  -0.31763205 ... -1.2876897  -1.1643226\n",
      "  -0.42393202]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.03811...2.3511746  -3.2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e2c820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.03811...2.3511746  -3.2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e2c820>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e2c850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e2c850>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e2c820>\n",
      "a = NDArray([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.0381164   0....15285783\n",
      "  -1.1825513 ]\n",
      " [-0.84942585  0.3735221  -0.31763205 ... -1.2876897  -1.1643226\n",
      "  -0.42393202]], device=cpu())\n",
      "b = NDArray([[-0.34177306  0.80441475  1.7493241  ...  2.3223765  -0.22549094\n",
      "   0.87994814]\n",
      " [ 0.62408024  0.17102477  1.....2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.0381164   0....15285783\n",
      "  -1.1825513 ]\n",
      " [-0.84942585  0.3735221  -0.31763205 ... -1.2876897  -1.1643226\n",
      "  -0.42393202]], device=cpu())\n",
      "b          = NDArray([[-0.34177306  0.80441475  1.7493241  ...  2.3223765  -0.22549094\n",
      "   0.87994814]\n",
      " [ 0.62408024  0.17102477  1.....2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e2c820>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-72-73-74] ___________________________\u001b[0m\n",
      "\n",
      "m = 72, n = 73, p = 74, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.64649886  1.7383245  -0.4676554  ...  0.0927      1.8445735\n",
      "   0.4557964 ]\n",
      " [-0.1723954  -1.1013563....1975574  -0.01113833\n",
      "   0.10480811]\n",
      " [ 1.7839968   3.4182303   0.24850914 ... -1.17262     0.42265594\n",
      "  -0.10205312]])\n",
      "B          = needle.Tensor([[-0.32170728 -1.1810572   1.2568568  ...  1.477072   -1.6280304\n",
      "  -0.18130198]\n",
      " [-0.9239965   0.764333 ...1.5999163   0.7752827\n",
      "  -0.3294631 ]\n",
      " [-0.30249047  1.397691    1.5853418  ... -1.6038519   0.11170492\n",
      "  -0.62114584]])\n",
      "_A         = array([[ 0.64649886,  1.7383245 , -0.4676554 , ...,  0.0927    ,\n",
      "         1.8445735 ,  0.4557964 ],\n",
      "       [-0.1723954...1],\n",
      "       [ 1.7839968 ,  3.4182303 ,  0.24850914, ..., -1.17262   ,\n",
      "         0.42265594, -0.10205312]], dtype=float32)\n",
      "_B         = array([[-0.32170728, -1.1810572 ,  1.2568568 , ...,  1.477072  ,\n",
      "        -1.6280304 , -0.18130198],\n",
      "       [-0.9239965... ],\n",
      "       [-0.30249047,  1.397691  ,  1.5853418 , ..., -1.6038519 ,\n",
      "         0.11170492, -0.62114584]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 72\n",
      "n          = 73\n",
      "p          = 74\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-0.32170728 -1.1810572   1.2568568  ...  1.477072   -1.6280304\n",
      "  -0.18130198]\n",
      " [-0.9239965   0.764333 ...1.5999163   0.7752827\n",
      "  -0.3294631 ]\n",
      " [-0.30249047  1.397691    1.5853418  ... -1.6038519   0.11170492\n",
      "  -0.62114584]])\n",
      "        self       = needle.Tensor([[ 0.64649886  1.7383245  -0.4676554  ...  0.0927      1.8445735\n",
      "   0.4557964 ]\n",
      " [-0.1723954  -1.1013563....1975574  -0.01113833\n",
      "   0.10480811]\n",
      " [ 1.7839968   3.4182303   0.24850914 ... -1.17262     0.42265594\n",
      "  -0.10205312]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.64649886  1.7383245  -0.4676554  ...  0.0927      1.8445735\n",
      "   0.4557964 ]\n",
      " [-0.1723954  -1.101356....5999163   0.7752827\n",
      "  -0.3294631 ]\n",
      " [-0.30249047  1.397691    1.5853418  ... -1.6038519   0.11170492\n",
      "  -0.62114584]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231d960d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.64649886  1.7383245  -0.4676554  ...  0.0927      1.8445735\n",
      "   0.4557964 ]\n",
      " [-0.1723954  -1.101356....5999163   0.7752827\n",
      "  -0.3294631 ]\n",
      " [-0.30249047  1.397691    1.5853418  ... -1.6038519   0.11170492\n",
      "  -0.62114584]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231d960d0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d96310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d96310>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231d960d0>\n",
      "a = NDArray([[ 0.64649886  1.7383245  -0.4676554  ...  0.0927      1.8445735\n",
      "   0.4557964 ]\n",
      " [-0.1723954  -1.1013563  -0.4...1113833\n",
      "   0.10480811]\n",
      " [ 1.7839968   3.4182303   0.24850914 ... -1.17262     0.42265594\n",
      "  -0.10205312]], device=cpu())\n",
      "b = NDArray([[-0.32170728 -1.1810572   1.2568568  ...  1.477072   -1.6280304\n",
      "  -0.18130198]\n",
      " [-0.9239965   0.764333   -1.3...7752827\n",
      "  -0.3294631 ]\n",
      " [-0.30249047  1.397691    1.5853418  ... -1.6038519   0.11170492\n",
      "  -0.62114584]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 0.64649886  1.7383245  -0.4676554  ...  0.0927      1.8445735\n",
      "   0.4557964 ]\n",
      " [-0.1723954  -1.1013563  -0.4...1113833\n",
      "   0.10480811]\n",
      " [ 1.7839968   3.4182303   0.24850914 ... -1.17262     0.42265594\n",
      "  -0.10205312]], device=cpu())\n",
      "b          = NDArray([[-0.32170728 -1.1810572   1.2568568  ...  1.477072   -1.6280304\n",
      "  -0.18130198]\n",
      " [-0.9239965   0.764333   -1.3...7752827\n",
      "  -0.3294631 ]\n",
      " [-0.30249047  1.397691    1.5853418  ... -1.6038519   0.11170492\n",
      "  -0.62114584]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231d960d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-74-73-72] ___________________________\u001b[0m\n",
      "\n",
      "m = 74, n = 73, p = 72, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-0.08567189  0.11218692 -1.9043691  ... -0.13281499  0.44163346\n",
      "  -0.8342325 ]\n",
      " [-0.6880523  -0.235206... -0.2993067   0.3354287\n",
      "   1.4878882 ]\n",
      " [-0.57864326  0.30619773  0.6196698  ...  0.7033562   1.564895\n",
      "  -2.079236  ]])\n",
      "B          = needle.Tensor([[-1.6981148e+00  2.0434272e+00  5.3226256e-01 ... -1.4473845e-03\n",
      "   1.0314587e+00  2.0466661e+00]\n",
      " [-8....01 -7.7169544e-01]\n",
      " [-2.1446531e-01  9.0655535e-01  1.7172813e+00 ... -1.6843086e+00\n",
      "  -1.0871487e+00  5.0207287e-01]])\n",
      "_A         = array([[-0.08567189,  0.11218692, -1.9043691 , ..., -0.13281499,\n",
      "         0.44163346, -0.8342325 ],\n",
      "       [-0.6880523... ],\n",
      "       [-0.57864326,  0.30619773,  0.6196698 , ...,  0.7033562 ,\n",
      "         1.564895  , -2.079236  ]], dtype=float32)\n",
      "_B         = array([[-1.6981148e+00,  2.0434272e+00,  5.3226256e-01, ...,\n",
      "        -1.4473845e-03,  1.0314587e+00,  2.0466661e+00],\n",
      "...6531e-01,  9.0655535e-01,  1.7172813e+00, ...,\n",
      "        -1.6843086e+00, -1.0871487e+00,  5.0207287e-01]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 74\n",
      "n          = 73\n",
      "p          = 72\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-1.6981148e+00  2.0434272e+00  5.3226256e-01 ... -1.4473845e-03\n",
      "   1.0314587e+00  2.0466661e+00]\n",
      " [-8....01 -7.7169544e-01]\n",
      " [-2.1446531e-01  9.0655535e-01  1.7172813e+00 ... -1.6843086e+00\n",
      "  -1.0871487e+00  5.0207287e-01]])\n",
      "        self       = needle.Tensor([[-0.08567189  0.11218692 -1.9043691  ... -0.13281499  0.44163346\n",
      "  -0.8342325 ]\n",
      " [-0.6880523  -0.235206... -0.2993067   0.3354287\n",
      "   1.4878882 ]\n",
      " [-0.57864326  0.30619773  0.6196698  ...  0.7033562   1.564895\n",
      "  -2.079236  ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.08567189  0.11218692 -1.9043691  ... -0.13281499  0.44163346\n",
      "  -0.8342325 ]\n",
      " [-0.6880523  -0.23520...1 -7.7169544e-01]\n",
      " [-2.1446531e-01  9.0655535e-01  1.7172813e+00 ... -1.6843086e+00\n",
      "  -1.0871487e+00  5.0207287e-01]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e3fd30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.08567189  0.11218692 -1.9043691  ... -0.13281499  0.44163346\n",
      "  -0.8342325 ]\n",
      " [-0.6880523  -0.23520...1 -7.7169544e-01]\n",
      " [-2.1446531e-01  9.0655535e-01  1.7172813e+00 ... -1.6843086e+00\n",
      "  -1.0871487e+00  5.0207287e-01]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e3fd30>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e3f730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e3f730>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e3fd30>\n",
      "a = NDArray([[-0.08567189  0.11218692 -1.9043691  ... -0.13281499  0.44163346\n",
      "  -0.8342325 ]\n",
      " [-0.6880523  -0.23520635 -0....0.3354287\n",
      "   1.4878882 ]\n",
      " [-0.57864326  0.30619773  0.6196698  ...  0.7033562   1.564895\n",
      "  -2.079236  ]], device=cpu())\n",
      "b = NDArray([[-1.6981148e+00  2.0434272e+00  5.3226256e-01 ... -1.4473845e-03\n",
      "   1.0314587e+00  2.0466661e+00]\n",
      " [-8.008250...-01]\n",
      " [-2.1446531e-01  9.0655535e-01  1.7172813e+00 ... -1.6843086e+00\n",
      "  -1.0871487e+00  5.0207287e-01]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-0.08567189  0.11218692 -1.9043691  ... -0.13281499  0.44163346\n",
      "  -0.8342325 ]\n",
      " [-0.6880523  -0.23520635 -0....0.3354287\n",
      "   1.4878882 ]\n",
      " [-0.57864326  0.30619773  0.6196698  ...  0.7033562   1.564895\n",
      "  -2.079236  ]], device=cpu())\n",
      "b          = NDArray([[-1.6981148e+00  2.0434272e+00  5.3226256e-01 ... -1.4473845e-03\n",
      "   1.0314587e+00  2.0466661e+00]\n",
      " [-8.008250...-01]\n",
      " [-2.1446531e-01  9.0655535e-01  1.7172813e+00 ... -1.6843086e+00\n",
      "  -1.0871487e+00  5.0207287e-01]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e3fd30>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_matmul[cpu-128-128-128] _________________________\u001b[0m\n",
      "\n",
      "m = 128, n = 128, p = 128, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924 ...0.20054239 -1.1254958\n",
      "  -0.12099399]\n",
      " [-0.30575415  1.0274688   2.1192799  ...  0.05281937  0.19285347\n",
      "  -1.8073474 ]])\n",
      "B          = needle.Tensor([[ 1.7749221  -1.5313677  -0.1591712  ... -0.13379207 -0.3885827\n",
      "   0.37725407]\n",
      " [-2.2680979  -0.3463737...1.8209276  -0.09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]])\n",
      "_A         = array([[ 0.22023457, -0.5929843 , -0.647192  , ...,  0.8990233 ,\n",
      "        -0.0346416 ,  0.39623448],\n",
      "       [-0.4756653...9],\n",
      "       [-0.30575415,  1.0274688 ,  2.1192799 , ...,  0.05281937,\n",
      "         0.19285347, -1.8073474 ]], dtype=float32)\n",
      "_B         = array([[ 1.7749221 , -1.5313677 , -0.1591712 , ..., -0.13379207,\n",
      "        -0.3885827 ,  0.37725407],\n",
      "       [-2.2680979...3],\n",
      "       [ 0.45524272, -0.43281022,  1.1288149 , ..., -0.5228368 ,\n",
      "        -0.2814408 , -0.6886872 ]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 128\n",
      "n          = 128\n",
      "p          = 128\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 1.7749221  -1.5313677  -0.1591712  ... -0.13379207 -0.3885827\n",
      "   0.37725407]\n",
      " [-2.2680979  -0.3463737...1.8209276  -0.09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]])\n",
      "        self       = needle.Tensor([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924 ...0.20054239 -1.1254958\n",
      "  -0.12099399]\n",
      " [-0.30575415  1.0274688   2.1192799  ...  0.05281937  0.19285347\n",
      "  -1.8073474 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924....8209276  -0.09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f22e2e5beb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924....8209276  -0.09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f22e2e5beb0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d96610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d96610>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f22e2e5beb0>\n",
      "a = NDArray([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924   -0.7...1254958\n",
      "  -0.12099399]\n",
      " [-0.30575415  1.0274688   2.1192799  ...  0.05281937  0.19285347\n",
      "  -1.8073474 ]], device=cpu())\n",
      "b = NDArray([[ 1.7749221  -1.5313677  -0.1591712  ... -0.13379207 -0.3885827\n",
      "   0.37725407]\n",
      " [-2.2680979  -0.34637374  0.8...09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924   -0.7...1254958\n",
      "  -0.12099399]\n",
      " [-0.30575415  1.0274688   2.1192799  ...  0.05281937  0.19285347\n",
      "  -1.8073474 ]], device=cpu())\n",
      "b          = NDArray([[ 1.7749221  -1.5313677  -0.1591712  ... -0.13379207 -0.3885827\n",
      "   0.37725407]\n",
      " [-2.2680979  -0.34637374  0.8...09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]], device=cpu())\n",
      "self       = <needle.ops.MatMul object at 0x7f22e2e5beb0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-16-16-16] __________________________\u001b[0m\n",
      "\n",
      "m = 16, n = 16, p = 16, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-4.8341087e-01  1.8093601e+00  1.6722593e+00  8.0163997e-01\n",
      "   1.5130348e+00  5.7170558e-01 -6.3828087...33733e-01  1.0966518e-03 -1.0496793e+00 -1.2557693e-01\n",
      "   1.1033653e+00  9.9985802e-01  1.3731679e+00 -6.6210198e-01]])\n",
      "B          = needle.Tensor([[-0.6730201  -1.2317069   0.6105233  -1.0086207   0.971255   -0.98045826\n",
      "  -0.87343013 -0.50210905  0.2...0449909 -0.3056322  -0.611826    0.9690245   1.0893643  -1.0063945\n",
      "  -0.397469   -1.5228411   0.27464268  0.8954157 ]])\n",
      "_A         = array([[-4.8341087e-01,  1.8093601e+00,  1.6722593e+00,  8.0163997e-01,\n",
      "         1.5130348e+00,  5.7170558e-01, -6.382...793e+00, -1.2557693e-01,\n",
      "         1.1033653e+00,  9.9985802e-01,  1.3731679e+00, -6.6210198e-01]],\n",
      "      dtype=float32)\n",
      "_B         = array([[-0.6730201 , -1.2317069 ,  0.6105233 , -1.0086207 ,  0.971255  ,\n",
      "        -0.98045826, -0.87343013, -0.50210905...690245 ,\n",
      "         1.0893643 , -1.0063945 , -0.397469  , -1.5228411 ,  0.27464268,\n",
      "         0.8954157 ]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 16\n",
      "n          = 16\n",
      "p          = 16\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-0.6730201  -1.2317069   0.6105233  -1.0086207   0.971255   -0.98045826\n",
      "  -0.87343013 -0.50210905  0.2...0449909 -0.3056322  -0.611826    0.9690245   1.0893643  -1.0063945\n",
      "  -0.397469   -1.5228411   0.27464268  0.8954157 ]])\n",
      "        self       = needle.Tensor([[-4.8341087e-01  1.8093601e+00  1.6722593e+00  8.0163997e-01\n",
      "   1.5130348e+00  5.7170558e-01 -6.3828087...33733e-01  1.0966518e-03 -1.0496793e+00 -1.2557693e-01\n",
      "   1.1033653e+00  9.9985802e-01  1.3731679e+00 -6.6210198e-01]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-4.8341087e-01  1.8093601e+00  1.6722593e+00  8.0163997e-01\n",
      "   1.5130348e+00  5.7170558e-01 -6.382808...449909 -0.3056322  -0.611826    0.9690245   1.0893643  -1.0063945\n",
      "  -0.397469   -1.5228411   0.27464268  0.8954157 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e3f460>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-4.8341087e-01  1.8093601e+00  1.6722593e+00  8.0163997e-01\n",
      "   1.5130348e+00  5.7170558e-01 -6.382808...449909 -0.3056322  -0.611826    0.9690245   1.0893643  -1.0063945\n",
      "  -0.397469   -1.5228411   0.27464268  0.8954157 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e3f460>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e3ffd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e3ffd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e3f460>\n",
      "a = NDArray([[-4.8341087e-01  1.8093601e+00  1.6722593e+00  8.0163997e-01\n",
      "   1.5130348e+00  5.7170558e-01 -6.3828087e-01  ...66518e-03 -1.0496793e+00 -1.2557693e-01\n",
      "   1.1033653e+00  9.9985802e-01  1.3731679e+00 -6.6210198e-01]], device=cuda())\n",
      "b = NDArray([[-0.6730201  -1.2317069   0.6105233  -1.0086207   0.971255   -0.98045826\n",
      "  -0.87343013 -0.50210905  0.2388575...322  -0.611826    0.9690245   1.0893643  -1.0063945\n",
      "  -0.397469   -1.5228411   0.27464268  0.8954157 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-4.8341087e-01  1.8093601e+00  1.6722593e+00  8.0163997e-01\n",
      "   1.5130348e+00  5.7170558e-01 -6.3828087e-01  ...66518e-03 -1.0496793e+00 -1.2557693e-01\n",
      "   1.1033653e+00  9.9985802e-01  1.3731679e+00 -6.6210198e-01]], device=cuda())\n",
      "b          = NDArray([[-0.6730201  -1.2317069   0.6105233  -1.0086207   0.971255   -0.98045826\n",
      "  -0.87343013 -0.50210905  0.2388575...322  -0.611826    0.9690245   1.0893643  -1.0063945\n",
      "  -0.397469   -1.5228411   0.27464268  0.8954157 ]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e3f460>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_matmul[cuda-8-8-8] ____________________________\u001b[0m\n",
      "\n",
      "m = 8, n = 8, p = 8, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-1.4774635  -1.6267474   0.49602756  0.31081805  1.2617579   0.89151007\n",
      "  -0.14909866 -1.3068166 ]\n",
      " [-...333 -1.5133457 ]\n",
      " [-0.05157733  1.380418   -1.8872807   0.7998044  -1.1721367  -0.11657883\n",
      "   1.6036352  -1.1328816 ]])\n",
      "B          = needle.Tensor([[ 1.0537726  -0.19507441  0.6123048  -1.2122457   0.46485847  0.58945036\n",
      "   2.0647578   1.6958039 ]\n",
      " [ ...503   0.12682916]\n",
      " [ 1.3682634   1.4154483  -1.5804107  -0.13925153  0.88675374  1.7259799\n",
      "  -0.5968434   2.8232105 ]])\n",
      "_A         = array([[-1.4774635 , -1.6267474 ,  0.49602756,  0.31081805,  1.2617579 ,\n",
      "         0.89151007, -0.14909866, -1.3068166 ...33,  1.380418  , -1.8872807 ,  0.7998044 , -1.1721367 ,\n",
      "        -0.11657883,  1.6036352 , -1.1328816 ]], dtype=float32)\n",
      "_B         = array([[ 1.0537726 , -0.19507441,  0.6123048 , -1.2122457 ,  0.46485847,\n",
      "         0.58945036,  2.0647578 ,  1.6958039 ...4 ,  1.4154483 , -1.5804107 , -0.13925153,  0.88675374,\n",
      "         1.7259799 , -0.5968434 ,  2.8232105 ]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 8\n",
      "n          = 8\n",
      "p          = 8\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 1.0537726  -0.19507441  0.6123048  -1.2122457   0.46485847  0.58945036\n",
      "   2.0647578   1.6958039 ]\n",
      " [ ...503   0.12682916]\n",
      " [ 1.3682634   1.4154483  -1.5804107  -0.13925153  0.88675374  1.7259799\n",
      "  -0.5968434   2.8232105 ]])\n",
      "        self       = needle.Tensor([[-1.4774635  -1.6267474   0.49602756  0.31081805  1.2617579   0.89151007\n",
      "  -0.14909866 -1.3068166 ]\n",
      " [-...333 -1.5133457 ]\n",
      " [-0.05157733  1.380418   -1.8872807   0.7998044  -1.1721367  -0.11657883\n",
      "   1.6036352  -1.1328816 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-1.4774635  -1.6267474   0.49602756  0.31081805  1.2617579   0.89151007\n",
      "  -0.14909866 -1.3068166 ]\n",
      " [...03   0.12682916]\n",
      " [ 1.3682634   1.4154483  -1.5804107  -0.13925153  0.88675374  1.7259799\n",
      "  -0.5968434   2.8232105 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231dd8dc0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-1.4774635  -1.6267474   0.49602756  0.31081805  1.2617579   0.89151007\n",
      "  -0.14909866 -1.3068166 ]\n",
      " [...03   0.12682916]\n",
      " [ 1.3682634   1.4154483  -1.5804107  -0.13925153  0.88675374  1.7259799\n",
      "  -0.5968434   2.8232105 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231dd8dc0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dd87f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dd87f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231dd8dc0>\n",
      "a = NDArray([[-1.4774635  -1.6267474   0.49602756  0.31081805  1.2617579   0.89151007\n",
      "  -0.14909866 -1.3068166 ]\n",
      " [-0.4996...]\n",
      " [-0.05157733  1.380418   -1.8872807   0.7998044  -1.1721367  -0.11657883\n",
      "   1.6036352  -1.1328816 ]], device=cuda())\n",
      "b = NDArray([[ 1.0537726  -0.19507441  0.6123048  -1.2122457   0.46485847  0.58945036\n",
      "   2.0647578   1.6958039 ]\n",
      " [ 0.2531...6]\n",
      " [ 1.3682634   1.4154483  -1.5804107  -0.13925153  0.88675374  1.7259799\n",
      "  -0.5968434   2.8232105 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-1.4774635  -1.6267474   0.49602756  0.31081805  1.2617579   0.89151007\n",
      "  -0.14909866 -1.3068166 ]\n",
      " [-0.4996...]\n",
      " [-0.05157733  1.380418   -1.8872807   0.7998044  -1.1721367  -0.11657883\n",
      "   1.6036352  -1.1328816 ]], device=cuda())\n",
      "b          = NDArray([[ 1.0537726  -0.19507441  0.6123048  -1.2122457   0.46485847  0.58945036\n",
      "   2.0647578   1.6958039 ]\n",
      " [ 0.2531...6]\n",
      " [ 1.3682634   1.4154483  -1.5804107  -0.13925153  0.88675374  1.7259799\n",
      "  -0.5968434   2.8232105 ]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231dd8dc0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_matmul[cuda-1-2-3] ____________________________\u001b[0m\n",
      "\n",
      "m = 1, n = 2, p = 3, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-0.0460759 -0.299862 ]])\n",
      "B          = needle.Tensor([[ 0.5341751   1.3190286   1.9854463 ]\n",
      " [-0.8265349   0.05200673 -2.2706025 ]])\n",
      "_A         = array([[-0.0460759, -0.299862 ]], dtype=float32)\n",
      "_B         = array([[ 0.5341751 ,  1.3190286 ,  1.9854463 ],\n",
      "       [-0.8265349 ,  0.05200673, -2.2706025 ]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 1\n",
      "n          = 2\n",
      "p          = 3\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 0.5341751   1.3190286   1.9854463 ]\n",
      " [-0.8265349   0.05200673 -2.2706025 ]])\n",
      "        self       = needle.Tensor([[-0.0460759 -0.299862 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.0460759 -0.299862 ]]), needle.Tensor([[ 0.5341751   1.3190286   1.9854463 ]\n",
      " [-0.8265349   0.05200673 -2.2706025 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e4bf70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.0460759 -0.299862 ]]), needle.Tensor([[ 0.5341751   1.3190286   1.9854463 ]\n",
      " [-0.8265349   0.05200673 -2.2706025 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e4bf70>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e4bd90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e4bd90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e4bf70>\n",
      "a = NDArray([[-0.0460759 -0.299862 ]], device=cuda())\n",
      "b = NDArray([[ 0.5341751   1.3190286   1.9854463 ]\n",
      " [-0.8265349   0.05200673 -2.2706025 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-0.0460759 -0.299862 ]], device=cuda())\n",
      "b          = NDArray([[ 0.5341751   1.3190286   1.9854463 ]\n",
      " [-0.8265349   0.05200673 -2.2706025 ]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e4bf70>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_matmul[cuda-3-4-5] ____________________________\u001b[0m\n",
      "\n",
      "m = 3, n = 4, p = 5, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.654816    0.72850287 -0.2106692  -0.17719436]\n",
      " [ 0.85219604  0.45814183 -0.09138661 -1.1754771 ]\n",
      " [-0.23682164  1.7254593   0.7422256  -0.0074945 ]])\n",
      "B          = needle.Tensor([[-0.3631913  -0.8601258   0.3114279   0.5743038   0.24980997]\n",
      " [ 0.6041445   1.287218   -0.06960689  0....47143 -1.6977823   0.49925917 -1.1749268  -1.5551547 ]\n",
      " [ 0.62649083  0.12255095 -1.0435718   0.24988598  0.3234629 ]])\n",
      "_A         = array([[ 0.654816  ,  0.72850287, -0.2106692 , -0.17719436],\n",
      "       [ 0.85219604,  0.45814183, -0.09138661, -1.1754771 ],\n",
      "       [-0.23682164,  1.7254593 ,  0.7422256 , -0.0074945 ]],\n",
      "      dtype=float32)\n",
      "_B         = array([[-0.3631913 , -0.8601258 ,  0.3114279 ,  0.5743038 ,  0.24980997],\n",
      "       [ 0.6041445 ,  1.287218  , -0.0696068...1749268 , -1.5551547 ],\n",
      "       [ 0.62649083,  0.12255095, -1.0435718 ,  0.24988598,  0.3234629 ]],\n",
      "      dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 3\n",
      "n          = 4\n",
      "p          = 5\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-0.3631913  -0.8601258   0.3114279   0.5743038   0.24980997]\n",
      " [ 0.6041445   1.287218   -0.06960689  0....47143 -1.6977823   0.49925917 -1.1749268  -1.5551547 ]\n",
      " [ 0.62649083  0.12255095 -1.0435718   0.24988598  0.3234629 ]])\n",
      "        self       = needle.Tensor([[ 0.654816    0.72850287 -0.2106692  -0.17719436]\n",
      " [ 0.85219604  0.45814183 -0.09138661 -1.1754771 ]\n",
      " [-0.23682164  1.7254593   0.7422256  -0.0074945 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.654816    0.72850287 -0.2106692  -0.17719436]\n",
      " [ 0.85219604  0.45814183 -0.09138661 -1.1754771 ]\n",
      " ...7143 -1.6977823   0.49925917 -1.1749268  -1.5551547 ]\n",
      " [ 0.62649083  0.12255095 -1.0435718   0.24988598  0.3234629 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231d38640>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.654816    0.72850287 -0.2106692  -0.17719436]\n",
      " [ 0.85219604  0.45814183 -0.09138661 -1.1754771 ]\n",
      " ...7143 -1.6977823   0.49925917 -1.1749268  -1.5551547 ]\n",
      " [ 0.62649083  0.12255095 -1.0435718   0.24988598  0.3234629 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231d38640>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d38af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d38af0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231d38640>\n",
      "a = NDArray([[ 0.654816    0.72850287 -0.2106692  -0.17719436]\n",
      " [ 0.85219604  0.45814183 -0.09138661 -1.1754771 ]\n",
      " [-0.23682164  1.7254593   0.7422256  -0.0074945 ]], device=cuda())\n",
      "b = NDArray([[-0.3631913  -0.8601258   0.3114279   0.5743038   0.24980997]\n",
      " [ 0.6041445   1.287218   -0.06960689  0.655798...3   0.49925917 -1.1749268  -1.5551547 ]\n",
      " [ 0.62649083  0.12255095 -1.0435718   0.24988598  0.3234629 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 0.654816    0.72850287 -0.2106692  -0.17719436]\n",
      " [ 0.85219604  0.45814183 -0.09138661 -1.1754771 ]\n",
      " [-0.23682164  1.7254593   0.7422256  -0.0074945 ]], device=cuda())\n",
      "b          = NDArray([[-0.3631913  -0.8601258   0.3114279   0.5743038   0.24980997]\n",
      " [ 0.6041445   1.287218   -0.06960689  0.655798...3   0.49925917 -1.1749268  -1.5551547 ]\n",
      " [ 0.62649083  0.12255095 -1.0435718   0.24988598  0.3234629 ]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231d38640>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_matmul[cuda-5-4-3] ____________________________\u001b[0m\n",
      "\n",
      "m = 5, n = 4, p = 3, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-1.3892012  -1.4697326   1.0070543   0.34883013]\n",
      " [-2.0843477  -0.24267545  0.11248029  1.987426  ]\n",
      " [...6  -1.36612   ]\n",
      " [ 0.9703613   0.0366048  -1.0864305   0.47813606]\n",
      " [-0.03617776  0.03262429 -1.8234262  -0.15546963]])\n",
      "B          = needle.Tensor([[ 0.7341658   0.8313993   0.08911297]\n",
      " [-0.10103321  0.25325453  0.22029683]\n",
      " [-1.2978022  -0.55815214  1.5079324 ]\n",
      " [ 0.19379255 -0.96344835 -0.30928165]])\n",
      "_A         = array([[-1.3892012 , -1.4697326 ,  1.0070543 ,  0.34883013],\n",
      "       [-2.0843477 , -0.24267545,  0.11248029,  1.987426 ...0366048 , -1.0864305 ,  0.47813606],\n",
      "       [-0.03617776,  0.03262429, -1.8234262 , -0.15546963]],\n",
      "      dtype=float32)\n",
      "_B         = array([[ 0.7341658 ,  0.8313993 ,  0.08911297],\n",
      "       [-0.10103321,  0.25325453,  0.22029683],\n",
      "       [-1.2978022 , -0.55815214,  1.5079324 ],\n",
      "       [ 0.19379255, -0.96344835, -0.30928165]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 5\n",
      "n          = 4\n",
      "p          = 3\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 0.7341658   0.8313993   0.08911297]\n",
      " [-0.10103321  0.25325453  0.22029683]\n",
      " [-1.2978022  -0.55815214  1.5079324 ]\n",
      " [ 0.19379255 -0.96344835 -0.30928165]])\n",
      "        self       = needle.Tensor([[-1.3892012  -1.4697326   1.0070543   0.34883013]\n",
      " [-2.0843477  -0.24267545  0.11248029  1.987426  ]\n",
      " [...6  -1.36612   ]\n",
      " [ 0.9703613   0.0366048  -1.0864305   0.47813606]\n",
      " [-0.03617776  0.03262429 -1.8234262  -0.15546963]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-1.3892012  -1.4697326   1.0070543   0.34883013]\n",
      " [-2.0843477  -0.24267545  0.11248029  1.987426  ]\n",
      " ... [-0.10103321  0.25325453  0.22029683]\n",
      " [-1.2978022  -0.55815214  1.5079324 ]\n",
      " [ 0.19379255 -0.96344835 -0.30928165]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231d60af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-1.3892012  -1.4697326   1.0070543   0.34883013]\n",
      " [-2.0843477  -0.24267545  0.11248029  1.987426  ]\n",
      " ... [-0.10103321  0.25325453  0.22029683]\n",
      " [-1.2978022  -0.55815214  1.5079324 ]\n",
      " [ 0.19379255 -0.96344835 -0.30928165]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231d60af0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d60b80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d60b80>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231d60af0>\n",
      "a = NDArray([[-1.3892012  -1.4697326   1.0070543   0.34883013]\n",
      " [-2.0843477  -0.24267545  0.11248029  1.987426  ]\n",
      " [-0.491...\n",
      " [ 0.9703613   0.0366048  -1.0864305   0.47813606]\n",
      " [-0.03617776  0.03262429 -1.8234262  -0.15546963]], device=cuda())\n",
      "b = NDArray([[ 0.7341658   0.8313993   0.08911297]\n",
      " [-0.10103321  0.25325453  0.22029683]\n",
      " [-1.2978022  -0.55815214  1.5079324 ]\n",
      " [ 0.19379255 -0.96344835 -0.30928165]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-1.3892012  -1.4697326   1.0070543   0.34883013]\n",
      " [-2.0843477  -0.24267545  0.11248029  1.987426  ]\n",
      " [-0.491...\n",
      " [ 0.9703613   0.0366048  -1.0864305   0.47813606]\n",
      " [-0.03617776  0.03262429 -1.8234262  -0.15546963]], device=cuda())\n",
      "b          = NDArray([[ 0.7341658   0.8313993   0.08911297]\n",
      " [-0.10103321  0.25325453  0.22029683]\n",
      " [-1.2978022  -0.55815214  1.5079324 ]\n",
      " [ 0.19379255 -0.96344835 -0.30928165]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231d60af0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-16-16-32] __________________________\u001b[0m\n",
      "\n",
      "m = 16, n = 16, p = 32, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-5.92951536e-01 -8.68785977e-01  1.28109825e+00 -2.13154539e-01\n",
      "   1.37239945e+00 -3.68608564e-01 -6.2...01  2.26365015e-01  7.11312830e-01 -2.11009383e+00\n",
      "  -2.48701707e-01 -7.93531001e-01  2.41460130e-01 -4.81520534e-01]])\n",
      "B          = needle.Tensor([[-7.37000585e-01  1.07231128e+00 -1.86253166e+00 -8.23480904e-01\n",
      "   1.96635008e-01 -1.68239748e+00 -6.3...00  4.87376332e-01 -4.97953176e-01 -1.90087244e-01\n",
      "  -8.86591554e-01  1.40486643e-01 -8.94032776e-01  2.30349619e-02]])\n",
      "_A         = array([[-5.92951536e-01, -8.68785977e-01,  1.28109825e+00,\n",
      "        -2.13154539e-01,  1.37239945e+00, -3.68608564e-01,\n",
      "..., -2.11009383e+00,\n",
      "        -2.48701707e-01, -7.93531001e-01,  2.41460130e-01,\n",
      "        -4.81520534e-01]], dtype=float32)\n",
      "_B         = array([[-7.37000585e-01,  1.07231128e+00, -1.86253166e+00,\n",
      "        -8.23480904e-01,  1.96635008e-01, -1.68239748e+00,\n",
      "...,\n",
      "        -1.90087244e-01, -8.86591554e-01,  1.40486643e-01,\n",
      "        -8.94032776e-01,  2.30349619e-02]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 16\n",
      "n          = 16\n",
      "p          = 32\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-7.37000585e-01  1.07231128e+00 -1.86253166e+00 -8.23480904e-01\n",
      "   1.96635008e-01 -1.68239748e+00 -6.3...00  4.87376332e-01 -4.97953176e-01 -1.90087244e-01\n",
      "  -8.86591554e-01  1.40486643e-01 -8.94032776e-01  2.30349619e-02]])\n",
      "        self       = needle.Tensor([[-5.92951536e-01 -8.68785977e-01  1.28109825e+00 -2.13154539e-01\n",
      "   1.37239945e+00 -3.68608564e-01 -6.2...01  2.26365015e-01  7.11312830e-01 -2.11009383e+00\n",
      "  -2.48701707e-01 -7.93531001e-01  2.41460130e-01 -4.81520534e-01]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-5.92951536e-01 -8.68785977e-01  1.28109825e+00 -2.13154539e-01\n",
      "   1.37239945e+00 -3.68608564e-01 -6....0  4.87376332e-01 -4.97953176e-01 -1.90087244e-01\n",
      "  -8.86591554e-01  1.40486643e-01 -8.94032776e-01  2.30349619e-02]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231de3370>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-5.92951536e-01 -8.68785977e-01  1.28109825e+00 -2.13154539e-01\n",
      "   1.37239945e+00 -3.68608564e-01 -6....0  4.87376332e-01 -4.97953176e-01 -1.90087244e-01\n",
      "  -8.86591554e-01  1.40486643e-01 -8.94032776e-01  2.30349619e-02]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231de3370>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231de34c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231de34c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231de3370>\n",
      "a = NDArray([[-5.92951536e-01 -8.68785977e-01  1.28109825e+00 -2.13154539e-01\n",
      "   1.37239945e+00 -3.68608564e-01 -6.2080677...-01  7.11312830e-01 -2.11009383e+00\n",
      "  -2.48701707e-01 -7.93531001e-01  2.41460130e-01 -4.81520534e-01]], device=cuda())\n",
      "b = NDArray([[-7.37000585e-01  1.07231128e+00 -1.86253166e+00 -8.23480904e-01\n",
      "   1.96635008e-01 -1.68239748e+00 -6.3273894...-01 -4.97953176e-01 -1.90087244e-01\n",
      "  -8.86591554e-01  1.40486643e-01 -8.94032776e-01  2.30349619e-02]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-5.92951536e-01 -8.68785977e-01  1.28109825e+00 -2.13154539e-01\n",
      "   1.37239945e+00 -3.68608564e-01 -6.2080677...-01  7.11312830e-01 -2.11009383e+00\n",
      "  -2.48701707e-01 -7.93531001e-01  2.41460130e-01 -4.81520534e-01]], device=cuda())\n",
      "b          = NDArray([[-7.37000585e-01  1.07231128e+00 -1.86253166e+00 -8.23480904e-01\n",
      "   1.96635008e-01 -1.68239748e+00 -6.3273894...-01 -4.97953176e-01 -1.90087244e-01\n",
      "  -8.86591554e-01  1.40486643e-01 -8.94032776e-01  2.30349619e-02]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231de3370>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-64-64-64] __________________________\u001b[0m\n",
      "\n",
      "m = 64, n = 64, p = 64, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-1.082387   -0.31285337  0.37914652 ... -0.41516542  1.9055135\n",
      "  -1.2175403 ]\n",
      " [-0.40063697  0.1319618...  1.6731632   2.133712\n",
      "  -0.02121179]\n",
      " [-0.6071943   0.6906254  -0.8257664  ... -0.33885276  0.4185762\n",
      "  -1.6399068 ]])\n",
      "B          = needle.Tensor([[ 0.41004443  1.1745555   0.80214846 ... -0.5857207   0.7658256\n",
      "   0.8750583 ]\n",
      " [ 1.1478539  -0.2587417...-0.20565246 -0.6102082\n",
      "  -0.8491065 ]\n",
      " [ 0.11291716  0.47895536 -1.1013322  ...  0.8595541  -1.5817857\n",
      "   0.8475244 ]])\n",
      "_A         = array([[-1.082387  , -0.31285337,  0.37914652, ..., -0.41516542,\n",
      "         1.9055135 , -1.2175403 ],\n",
      "       [-0.4006369...9],\n",
      "       [-0.6071943 ,  0.6906254 , -0.8257664 , ..., -0.33885276,\n",
      "         0.4185762 , -1.6399068 ]], dtype=float32)\n",
      "_B         = array([[ 0.41004443,  1.1745555 ,  0.80214846, ..., -0.5857207 ,\n",
      "         0.7658256 ,  0.8750583 ],\n",
      "       [ 1.1478539... ],\n",
      "       [ 0.11291716,  0.47895536, -1.1013322 , ...,  0.8595541 ,\n",
      "        -1.5817857 ,  0.8475244 ]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 64\n",
      "n          = 64\n",
      "p          = 64\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 0.41004443  1.1745555   0.80214846 ... -0.5857207   0.7658256\n",
      "   0.8750583 ]\n",
      " [ 1.1478539  -0.2587417...-0.20565246 -0.6102082\n",
      "  -0.8491065 ]\n",
      " [ 0.11291716  0.47895536 -1.1013322  ...  0.8595541  -1.5817857\n",
      "   0.8475244 ]])\n",
      "        self       = needle.Tensor([[-1.082387   -0.31285337  0.37914652 ... -0.41516542  1.9055135\n",
      "  -1.2175403 ]\n",
      " [-0.40063697  0.1319618...  1.6731632   2.133712\n",
      "  -0.02121179]\n",
      " [-0.6071943   0.6906254  -0.8257664  ... -0.33885276  0.4185762\n",
      "  -1.6399068 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-1.082387   -0.31285337  0.37914652 ... -0.41516542  1.9055135\n",
      "  -1.2175403 ]\n",
      " [-0.40063697  0.131961...0.20565246 -0.6102082\n",
      "  -0.8491065 ]\n",
      " [ 0.11291716  0.47895536 -1.1013322  ...  0.8595541  -1.5817857\n",
      "   0.8475244 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231dd2af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-1.082387   -0.31285337  0.37914652 ... -0.41516542  1.9055135\n",
      "  -1.2175403 ]\n",
      " [-0.40063697  0.131961...0.20565246 -0.6102082\n",
      "  -0.8491065 ]\n",
      " [ 0.11291716  0.47895536 -1.1013322  ...  0.8595541  -1.5817857\n",
      "   0.8475244 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231dd2af0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dd27f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dd27f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231dd2af0>\n",
      "a = NDArray([[-1.082387   -0.31285337  0.37914652 ... -0.41516542  1.9055135\n",
      "  -1.2175403 ]\n",
      " [-0.40063697  0.13196188 -0.4....133712\n",
      "  -0.02121179]\n",
      " [-0.6071943   0.6906254  -0.8257664  ... -0.33885276  0.4185762\n",
      "  -1.6399068 ]], device=cuda())\n",
      "b = NDArray([[ 0.41004443  1.1745555   0.80214846 ... -0.5857207   0.7658256\n",
      "   0.8750583 ]\n",
      " [ 1.1478539  -0.25874174 -0.0...6102082\n",
      "  -0.8491065 ]\n",
      " [ 0.11291716  0.47895536 -1.1013322  ...  0.8595541  -1.5817857\n",
      "   0.8475244 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-1.082387   -0.31285337  0.37914652 ... -0.41516542  1.9055135\n",
      "  -1.2175403 ]\n",
      " [-0.40063697  0.13196188 -0.4....133712\n",
      "  -0.02121179]\n",
      " [-0.6071943   0.6906254  -0.8257664  ... -0.33885276  0.4185762\n",
      "  -1.6399068 ]], device=cuda())\n",
      "b          = NDArray([[ 0.41004443  1.1745555   0.80214846 ... -0.5857207   0.7658256\n",
      "   0.8750583 ]\n",
      " [ 1.1478539  -0.25874174 -0.0...6102082\n",
      "  -0.8491065 ]\n",
      " [ 0.11291716  0.47895536 -1.1013322  ...  0.8595541  -1.5817857\n",
      "   0.8475244 ]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231dd2af0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-72-72-72] __________________________\u001b[0m\n",
      "\n",
      "m = 72, n = 72, p = 72, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-0.14028323  0.60604614  0.04811767 ...  0.51525635  0.45924923\n",
      "   1.9541698 ]\n",
      " [-1.1255426  -1.340524...-0.30294827  0.4433803\n",
      "   0.5457909 ]\n",
      " [-0.13650139  0.41142735 -1.6502085  ... -2.5761743   1.4478314\n",
      "   2.2190225 ]])\n",
      "B          = needle.Tensor([[ 0.24280068  0.02310721  1.9812624  ... -1.1015782  -0.3129008\n",
      "   0.5849371 ]\n",
      " [ 1.4762851   1.9007288...-0.07453733 -1.0112509\n",
      "  -0.9567248 ]\n",
      " [ 0.76414275  0.25707817 -2.0578747  ...  0.6905931  -1.3811514\n",
      "   0.6022044 ]])\n",
      "_A         = array([[-0.14028323,  0.60604614,  0.04811767, ...,  0.51525635,\n",
      "         0.45924923,  1.9541698 ],\n",
      "       [-1.1255426... ],\n",
      "       [-0.13650139,  0.41142735, -1.6502085 , ..., -2.5761743 ,\n",
      "         1.4478314 ,  2.2190225 ]], dtype=float32)\n",
      "_B         = array([[ 0.24280068,  0.02310721,  1.9812624 , ..., -1.1015782 ,\n",
      "        -0.3129008 ,  0.5849371 ],\n",
      "       [ 1.4762851... ],\n",
      "       [ 0.76414275,  0.25707817, -2.0578747 , ...,  0.6905931 ,\n",
      "        -1.3811514 ,  0.6022044 ]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 72\n",
      "n          = 72\n",
      "p          = 72\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 0.24280068  0.02310721  1.9812624  ... -1.1015782  -0.3129008\n",
      "   0.5849371 ]\n",
      " [ 1.4762851   1.9007288...-0.07453733 -1.0112509\n",
      "  -0.9567248 ]\n",
      " [ 0.76414275  0.25707817 -2.0578747  ...  0.6905931  -1.3811514\n",
      "   0.6022044 ]])\n",
      "        self       = needle.Tensor([[-0.14028323  0.60604614  0.04811767 ...  0.51525635  0.45924923\n",
      "   1.9541698 ]\n",
      " [-1.1255426  -1.340524...-0.30294827  0.4433803\n",
      "   0.5457909 ]\n",
      " [-0.13650139  0.41142735 -1.6502085  ... -2.5761743   1.4478314\n",
      "   2.2190225 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.14028323  0.60604614  0.04811767 ...  0.51525635  0.45924923\n",
      "   1.9541698 ]\n",
      " [-1.1255426  -1.34052...0.07453733 -1.0112509\n",
      "  -0.9567248 ]\n",
      " [ 0.76414275  0.25707817 -2.0578747  ...  0.6905931  -1.3811514\n",
      "   0.6022044 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231d59d60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.14028323  0.60604614  0.04811767 ...  0.51525635  0.45924923\n",
      "   1.9541698 ]\n",
      " [-1.1255426  -1.34052...0.07453733 -1.0112509\n",
      "  -0.9567248 ]\n",
      " [ 0.76414275  0.25707817 -2.0578747  ...  0.6905931  -1.3811514\n",
      "   0.6022044 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231d59d60>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d59c70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231d59c70>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231d59d60>\n",
      "a = NDArray([[-0.14028323  0.60604614  0.04811767 ...  0.51525635  0.45924923\n",
      "   1.9541698 ]\n",
      " [-1.1255426  -1.3405246  -0....4433803\n",
      "   0.5457909 ]\n",
      " [-0.13650139  0.41142735 -1.6502085  ... -2.5761743   1.4478314\n",
      "   2.2190225 ]], device=cuda())\n",
      "b = NDArray([[ 0.24280068  0.02310721  1.9812624  ... -1.1015782  -0.3129008\n",
      "   0.5849371 ]\n",
      " [ 1.4762851   1.9007288   0.0...0112509\n",
      "  -0.9567248 ]\n",
      " [ 0.76414275  0.25707817 -2.0578747  ...  0.6905931  -1.3811514\n",
      "   0.6022044 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-0.14028323  0.60604614  0.04811767 ...  0.51525635  0.45924923\n",
      "   1.9541698 ]\n",
      " [-1.1255426  -1.3405246  -0....4433803\n",
      "   0.5457909 ]\n",
      " [-0.13650139  0.41142735 -1.6502085  ... -2.5761743   1.4478314\n",
      "   2.2190225 ]], device=cuda())\n",
      "b          = NDArray([[ 0.24280068  0.02310721  1.9812624  ... -1.1015782  -0.3129008\n",
      "   0.5849371 ]\n",
      " [ 1.4762851   1.9007288   0.0...0112509\n",
      "  -0.9567248 ]\n",
      " [ 0.76414275  0.25707817 -2.0578747  ...  0.6905931  -1.3811514\n",
      "   0.6022044 ]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231d59d60>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-72-73-74] __________________________\u001b[0m\n",
      "\n",
      "m = 72, n = 73, p = 74, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-2.2869356   2.4410696  -0.06609856 ... -0.6628585   1.2714901\n",
      "  -1.6431442 ]\n",
      " [-0.6135641  -2.632752 ... 0.3799703  -1.2624217\n",
      "  -1.4366313 ]\n",
      " [ 0.37932992  0.2737079   1.8354076  ...  0.7942683   2.1143708\n",
      "   0.5757829 ]])\n",
      "B          = needle.Tensor([[-0.81970483 -0.62832344  0.58655894 ... -0.7188211  -0.16015641\n",
      "  -0.38464722]\n",
      " [-0.98472214 -0.908692...0.6638228  -0.0929161\n",
      "  -1.6803774 ]\n",
      " [-0.8729308  -0.37538484  0.90310943 ...  0.34306654 -0.10175896\n",
      "   1.9507737 ]])\n",
      "_A         = array([[-2.2869356 ,  2.4410696 , -0.06609856, ..., -0.6628585 ,\n",
      "         1.2714901 , -1.6431442 ],\n",
      "       [-0.6135641... ],\n",
      "       [ 0.37932992,  0.2737079 ,  1.8354076 , ...,  0.7942683 ,\n",
      "         2.1143708 ,  0.5757829 ]], dtype=float32)\n",
      "_B         = array([[-0.81970483, -0.62832344,  0.58655894, ..., -0.7188211 ,\n",
      "        -0.16015641, -0.38464722],\n",
      "       [-0.9847221... ],\n",
      "       [-0.8729308 , -0.37538484,  0.90310943, ...,  0.34306654,\n",
      "        -0.10175896,  1.9507737 ]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 72\n",
      "n          = 73\n",
      "p          = 74\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-0.81970483 -0.62832344  0.58655894 ... -0.7188211  -0.16015641\n",
      "  -0.38464722]\n",
      " [-0.98472214 -0.908692...0.6638228  -0.0929161\n",
      "  -1.6803774 ]\n",
      " [-0.8729308  -0.37538484  0.90310943 ...  0.34306654 -0.10175896\n",
      "   1.9507737 ]])\n",
      "        self       = needle.Tensor([[-2.2869356   2.4410696  -0.06609856 ... -0.6628585   1.2714901\n",
      "  -1.6431442 ]\n",
      " [-0.6135641  -2.632752 ... 0.3799703  -1.2624217\n",
      "  -1.4366313 ]\n",
      " [ 0.37932992  0.2737079   1.8354076  ...  0.7942683   2.1143708\n",
      "   0.5757829 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-2.2869356   2.4410696  -0.06609856 ... -0.6628585   1.2714901\n",
      "  -1.6431442 ]\n",
      " [-0.6135641  -2.632752....6638228  -0.0929161\n",
      "  -1.6803774 ]\n",
      " [-0.8729308  -0.37538484  0.90310943 ...  0.34306654 -0.10175896\n",
      "   1.9507737 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e12580>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-2.2869356   2.4410696  -0.06609856 ... -0.6628585   1.2714901\n",
      "  -1.6431442 ]\n",
      " [-0.6135641  -2.632752....6638228  -0.0929161\n",
      "  -1.6803774 ]\n",
      " [-0.8729308  -0.37538484  0.90310943 ...  0.34306654 -0.10175896\n",
      "   1.9507737 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e12580>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e12b80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e12b80>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e12580>\n",
      "a = NDArray([[-2.2869356   2.4410696  -0.06609856 ... -0.6628585   1.2714901\n",
      "  -1.6431442 ]\n",
      " [-0.6135641  -2.632752    0.2...2624217\n",
      "  -1.4366313 ]\n",
      " [ 0.37932992  0.2737079   1.8354076  ...  0.7942683   2.1143708\n",
      "   0.5757829 ]], device=cuda())\n",
      "b = NDArray([[-0.81970483 -0.62832344  0.58655894 ... -0.7188211  -0.16015641\n",
      "  -0.38464722]\n",
      " [-0.98472214 -0.9086928  -1....929161\n",
      "  -1.6803774 ]\n",
      " [-0.8729308  -0.37538484  0.90310943 ...  0.34306654 -0.10175896\n",
      "   1.9507737 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-2.2869356   2.4410696  -0.06609856 ... -0.6628585   1.2714901\n",
      "  -1.6431442 ]\n",
      " [-0.6135641  -2.632752    0.2...2624217\n",
      "  -1.4366313 ]\n",
      " [ 0.37932992  0.2737079   1.8354076  ...  0.7942683   2.1143708\n",
      "   0.5757829 ]], device=cuda())\n",
      "b          = NDArray([[-0.81970483 -0.62832344  0.58655894 ... -0.7188211  -0.16015641\n",
      "  -0.38464722]\n",
      " [-0.98472214 -0.9086928  -1....929161\n",
      "  -1.6803774 ]\n",
      " [-0.8729308  -0.37538484  0.90310943 ...  0.34306654 -0.10175896\n",
      "   1.9507737 ]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e12580>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-74-73-72] __________________________\u001b[0m\n",
      "\n",
      "m = 74, n = 73, p = 72, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 2.5433717   2.202355   -1.7542894  ...  0.4746837   0.291169\n",
      "   0.632095  ]\n",
      " [ 0.50233406 -0.06469969...0.4369837   0.14232586\n",
      "   0.44178468]\n",
      " [ 0.3124429   0.67045975 -0.2541804  ...  0.8254428  -0.9933159\n",
      "  -1.3417903 ]])\n",
      "B          = needle.Tensor([[ 0.86880785 -0.70741963 -0.14541002 ...  0.33905905  0.03319501\n",
      "  -0.30614465]\n",
      " [ 0.3715162   2.018187...0.6139773   0.9911844\n",
      "  -0.59475243]\n",
      " [ 1.4325385   0.01491717 -0.4534368  ... -1.5561616  -0.32822824\n",
      "  -0.85069335]])\n",
      "_A         = array([[ 2.5433717 ,  2.202355  , -1.7542894 , ...,  0.4746837 ,\n",
      "         0.291169  ,  0.632095  ],\n",
      "       [ 0.5023340...8],\n",
      "       [ 0.3124429 ,  0.67045975, -0.2541804 , ...,  0.8254428 ,\n",
      "        -0.9933159 , -1.3417903 ]], dtype=float32)\n",
      "_B         = array([[ 0.86880785, -0.70741963, -0.14541002, ...,  0.33905905,\n",
      "         0.03319501, -0.30614465],\n",
      "       [ 0.3715162...3],\n",
      "       [ 1.4325385 ,  0.01491717, -0.4534368 , ..., -1.5561616 ,\n",
      "        -0.32822824, -0.85069335]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 74\n",
      "n          = 73\n",
      "p          = 72\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 0.86880785 -0.70741963 -0.14541002 ...  0.33905905  0.03319501\n",
      "  -0.30614465]\n",
      " [ 0.3715162   2.018187...0.6139773   0.9911844\n",
      "  -0.59475243]\n",
      " [ 1.4325385   0.01491717 -0.4534368  ... -1.5561616  -0.32822824\n",
      "  -0.85069335]])\n",
      "        self       = needle.Tensor([[ 2.5433717   2.202355   -1.7542894  ...  0.4746837   0.291169\n",
      "   0.632095  ]\n",
      " [ 0.50233406 -0.06469969...0.4369837   0.14232586\n",
      "   0.44178468]\n",
      " [ 0.3124429   0.67045975 -0.2541804  ...  0.8254428  -0.9933159\n",
      "  -1.3417903 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 2.5433717   2.202355   -1.7542894  ...  0.4746837   0.291169\n",
      "   0.632095  ]\n",
      " [ 0.50233406 -0.0646996....6139773   0.9911844\n",
      "  -0.59475243]\n",
      " [ 1.4325385   0.01491717 -0.4534368  ... -1.5561616  -0.32822824\n",
      "  -0.85069335]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231dcc6a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 2.5433717   2.202355   -1.7542894  ...  0.4746837   0.291169\n",
      "   0.632095  ]\n",
      " [ 0.50233406 -0.0646996....6139773   0.9911844\n",
      "  -0.59475243]\n",
      " [ 1.4325385   0.01491717 -0.4534368  ... -1.5561616  -0.32822824\n",
      "  -0.85069335]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231dcc6a0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dcc880>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231dcc880>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231dcc6a0>\n",
      "a = NDArray([[ 2.5433717   2.202355   -1.7542894  ...  0.4746837   0.291169\n",
      "   0.632095  ]\n",
      " [ 0.50233406 -0.06469969 -1.81...4232586\n",
      "   0.44178468]\n",
      " [ 0.3124429   0.67045975 -0.2541804  ...  0.8254428  -0.9933159\n",
      "  -1.3417903 ]], device=cuda())\n",
      "b = NDArray([[ 0.86880785 -0.70741963 -0.14541002 ...  0.33905905  0.03319501\n",
      "  -0.30614465]\n",
      " [ 0.3715162   2.018187    0....911844\n",
      "  -0.59475243]\n",
      " [ 1.4325385   0.01491717 -0.4534368  ... -1.5561616  -0.32822824\n",
      "  -0.85069335]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 2.5433717   2.202355   -1.7542894  ...  0.4746837   0.291169\n",
      "   0.632095  ]\n",
      " [ 0.50233406 -0.06469969 -1.81...4232586\n",
      "   0.44178468]\n",
      " [ 0.3124429   0.67045975 -0.2541804  ...  0.8254428  -0.9933159\n",
      "  -1.3417903 ]], device=cuda())\n",
      "b          = NDArray([[ 0.86880785 -0.70741963 -0.14541002 ...  0.33905905  0.03319501\n",
      "  -0.30614465]\n",
      " [ 0.3715162   2.018187    0....911844\n",
      "  -0.59475243]\n",
      " [ 1.4325385   0.01491717 -0.4534368  ... -1.5561616  -0.32822824\n",
      "  -0.85069335]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231dcc6a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_matmul[cuda-128-128-128] _________________________\u001b[0m\n",
      "\n",
      "m = 128, n = 128, p = 128, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 1.9537501   0.05903736 -1.2711045  ...  0.22699478 -0.10282926\n",
      "  -0.18651716]\n",
      " [-0.2311179  -0.412531...-0.11042102 -1.5155143\n",
      "   0.36630192]\n",
      " [ 0.06558133 -1.1310216  -1.9421903  ... -1.8011059   1.3072039\n",
      "   0.5935067 ]])\n",
      "B          = needle.Tensor([[ 0.9572396  -0.46439824 -1.6827878  ... -0.11662703 -1.5686997\n",
      "   0.01031934]\n",
      " [-0.18556063  0.3246031...1.5828687  -0.81532604\n",
      "  -0.40916657]\n",
      " [-0.8687678  -1.4076406   2.1396523  ...  1.6590946  -0.4116172\n",
      "  -0.4557549 ]])\n",
      "_A         = array([[ 1.9537501 ,  0.05903736, -1.2711045 , ...,  0.22699478,\n",
      "        -0.10282926, -0.18651716],\n",
      "       [-0.2311179...2],\n",
      "       [ 0.06558133, -1.1310216 , -1.9421903 , ..., -1.8011059 ,\n",
      "         1.3072039 ,  0.5935067 ]], dtype=float32)\n",
      "_B         = array([[ 0.9572396 , -0.46439824, -1.6827878 , ..., -0.11662703,\n",
      "        -1.5686997 ,  0.01031934],\n",
      "       [-0.1855606...7],\n",
      "       [-0.8687678 , -1.4076406 ,  2.1396523 , ...,  1.6590946 ,\n",
      "        -0.4116172 , -0.4557549 ]], dtype=float32)\n",
      "device     = cuda()\n",
      "m          = 128\n",
      "n          = 128\n",
      "p          = 128\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:310: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 0.9572396  -0.46439824 -1.6827878  ... -0.11662703 -1.5686997\n",
      "   0.01031934]\n",
      " [-0.18556063  0.3246031...1.5828687  -0.81532604\n",
      "  -0.40916657]\n",
      " [-0.8687678  -1.4076406   2.1396523  ...  1.6590946  -0.4116172\n",
      "  -0.4557549 ]])\n",
      "        self       = needle.Tensor([[ 1.9537501   0.05903736 -1.2711045  ...  0.22699478 -0.10282926\n",
      "  -0.18651716]\n",
      " [-0.2311179  -0.412531...-0.11042102 -1.5155143\n",
      "   0.36630192]\n",
      " [ 0.06558133 -1.1310216  -1.9421903  ... -1.8011059   1.3072039\n",
      "   0.5935067 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.9537501   0.05903736 -1.2711045  ...  0.22699478 -0.10282926\n",
      "  -0.18651716]\n",
      " [-0.2311179  -0.41253....5828687  -0.81532604\n",
      "  -0.40916657]\n",
      " [-0.8687678  -1.4076406   2.1396523  ...  1.6590946  -0.4116172\n",
      "  -0.4557549 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7f2231e41100>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.9537501   0.05903736 -1.2711045  ...  0.22699478 -0.10282926\n",
      "  -0.18651716]\n",
      " [-0.2311179  -0.41253....5828687  -0.81532604\n",
      "  -0.40916657]\n",
      " [-0.8687678  -1.4076406   2.1396523  ...  1.6590946  -0.4116172\n",
      "  -0.4557549 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7f2231e41100>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e41d60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'matmul'\") raised in repr()] Tensor object at 0x7f2231e41d60>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.MatMul object at 0x7f2231e41100>\n",
      "a = NDArray([[ 1.9537501   0.05903736 -1.2711045  ...  0.22699478 -0.10282926\n",
      "  -0.18651716]\n",
      " [-0.2311179  -0.41253147 -1....5155143\n",
      "   0.36630192]\n",
      " [ 0.06558133 -1.1310216  -1.9421903  ... -1.8011059   1.3072039\n",
      "   0.5935067 ]], device=cuda())\n",
      "b = NDArray([[ 0.9572396  -0.46439824 -1.6827878  ... -0.11662703 -1.5686997\n",
      "   0.01031934]\n",
      " [-0.18556063  0.3246031  -0.4...1532604\n",
      "  -0.40916657]\n",
      " [-0.8687678  -1.4076406   2.1396523  ...  1.6590946  -0.4116172\n",
      "  -0.4557549 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.9537501   0.05903736 -1.2711045  ...  0.22699478 -0.10282926\n",
      "  -0.18651716]\n",
      " [-0.2311179  -0.41253147 -1....5155143\n",
      "   0.36630192]\n",
      " [ 0.06558133 -1.1310216  -1.9421903  ... -1.8011059   1.3072039\n",
      "   0.5935067 ]], device=cuda())\n",
      "b          = NDArray([[ 0.9572396  -0.46439824 -1.6827878  ... -0.11662703 -1.5686997\n",
      "   0.01031934]\n",
      " [-0.18556063  0.3246031  -0.4...1532604\n",
      "  -0.40916657]\n",
      " [-0.8687678  -1.4076406   2.1396523  ...  1.6590946  -0.4116172\n",
      "  -0.4557549 ]], device=cuda())\n",
      "self       = <needle.ops.MatMul object at 0x7f2231e41100>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:304: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_power[cpu-shape0] ____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.9513445]]])\n",
      "_A         = array([[[-0.9513445]]], dtype=float32)\n",
      "_B         = 0\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:102: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:288: in __pow__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\n",
      "        other      = 0\n",
      "        self       = needle.Tensor([[[-0.9513445]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.9513445]]]),)\n",
      "        self       = <needle.ops.PowerScalar object at 0x7f2231e4bf10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.9513445]]]),)\n",
      "        op         = <needle.ops.PowerScalar object at 0x7f2231e4bf10>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f2231e4b250>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f2231e4b250>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.PowerScalar object at 0x7f2231e4bf10>\n",
      "a = NDArray([[[-0.9513445]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.power(a, \u001b[96mself\u001b[39;49;00m.scalar)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.9513445]]], device=cpu())\n",
      "self       = <needle.ops.PowerScalar object at 0x7f2231e4bf10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:135: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_power[cpu-shape1] ____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9.0...+00  5.7307726e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]])\n",
      "_A         = array([[[-5.0679171e-01,  3.2509923e-01, -9.7235195e-02,  8.6115736e-01,\n",
      "          1.5928187e+00, -1.6549641e-01],\n",
      "   ...1119346e-01,  3.5744989e-01, -5.4865015e-01, -1.9209417e+00,\n",
      "          3.4405407e-01, -3.6635080e-01]]], dtype=float32)\n",
      "_B         = 0\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:102: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:288: in __pow__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\n",
      "        other      = 0\n",
      "        self       = needle.Tensor([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9.0...+00  5.7307726e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9....0  5.7307726e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]]),)\n",
      "        self       = <needle.ops.PowerScalar object at 0x7f2231dde550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9....0  5.7307726e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]]),)\n",
      "        op         = <needle.ops.PowerScalar object at 0x7f2231dde550>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f2231dde610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f2231dde610>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.PowerScalar object at 0x7f2231dde550>\n",
      "a = NDArray([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9.0205765...e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.power(a, \u001b[96mself\u001b[39;49;00m.scalar)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9.0205765...e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]], device=cpu())\n",
      "self       = <needle.ops.PowerScalar object at 0x7f2231dde550>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:135: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_power[cuda-shape0] ____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[1.171145]]])\n",
      "_A         = array([[[1.171145]]], dtype=float32)\n",
      "_B         = 0\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:102: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:288: in __pow__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\n",
      "        other      = 0\n",
      "        self       = needle.Tensor([[[1.171145]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[1.171145]]]),)\n",
      "        self       = <needle.ops.PowerScalar object at 0x7f2239a66ee0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[1.171145]]]),)\n",
      "        op         = <needle.ops.PowerScalar object at 0x7f2239a66ee0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f2239a66100>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f2239a66100>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.PowerScalar object at 0x7f2239a66ee0>\n",
      "a = NDArray([[[1.171145]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.power(a, \u001b[96mself\u001b[39;49;00m.scalar)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[1.171145]]], device=cuda())\n",
      "self       = <needle.ops.PowerScalar object at 0x7f2239a66ee0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:135: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_power[cuda-shape1] ____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.9756789...0.01338127  0.37821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]])\n",
      "_A         = array([[[-0.6280028 , -0.43987197,  2.9139524 ,  0.05009432,\n",
      "         -0.73577106, -0.47775924],\n",
      "        [-1.7431105 ,...16 ],\n",
      "        [-0.8517918 , -1.9387237 ,  0.42015556, -0.24831858,\n",
      "         -1.0194412 ,  0.38210192]]], dtype=float32)\n",
      "_B         = 0\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:102: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:288: in __pow__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\n",
      "        other      = 0\n",
      "        self       = needle.Tensor([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.9756789...0.01338127  0.37821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.975678...01338127  0.37821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]]),)\n",
      "        self       = <needle.ops.PowerScalar object at 0x7f2231e6af10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.975678...01338127  0.37821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]]),)\n",
      "        op         = <needle.ops.PowerScalar object at 0x7f2231e6af10>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f2231e6a190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f2231e6a190>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.PowerScalar object at 0x7f2231e6af10>\n",
      "a = NDArray([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.9756789  -0.3...7821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.power(a, \u001b[96mself\u001b[39;49;00m.scalar)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.9756789  -0.3...7821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]], device=cuda())\n",
      "self       = <needle.ops.PowerScalar object at 0x7f2231e6af10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:135: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cpu-shape0] _____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.333942]]])\n",
      "_A         = array([[[-1.333942]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:466: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-1.333942]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.333942]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7f2231e2c760>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.333942]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7f2231e2c760>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231e2c820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231e2c820>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7f2231e2c760>\n",
      "a = NDArray([[[-1.333942]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       array_api.ewise_tanh(a)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.333942]]], device=cpu())\n",
      "self       = <needle.ops.Tanh object at 0x7f2231e2c760>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:453: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cpu-shape1] _____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      " ...50593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]])\n",
      "_A         = array([[[ 4.68197703e-01,  2.24348330e+00,  4.72604215e-01,\n",
      "          1.18212029e-01, -6.31235898e-01,  1.17569625e+00...,  1.98646748e+00, -1.20881248e+00,\n",
      "          8.64627600e-01,  1.01337186e-03, -9.01962698e-01]]],\n",
      "      dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:466: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      " ...50593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "...593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7f2231e180a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "...593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7f2231e180a0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231e18670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231e18670>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7f2231e180a0>\n",
      "a = NDArray([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "  [-5.4...  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       array_api.ewise_tanh(a)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "  [-5.4...  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]], device=cpu())\n",
      "self       = <needle.ops.Tanh object at 0x7f2231e180a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:453: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape0] ____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-2.3530958]]])\n",
      "_A         = array([[[-2.3530958]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:466: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-2.3530958]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-2.3530958]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7f2231e6f910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-2.3530958]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7f2231e6f910>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231e6f0a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231e6f0a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7f2231e6f910>\n",
      "a = NDArray([[[-2.3530958]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       array_api.ewise_tanh(a)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-2.3530958]]], device=cuda())\n",
      "self       = <needle.ops.Tanh object at 0x7f2231e6f910>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:453: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape1] ____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628...0.8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]])\n",
      "_A         = array([[[-0.5291037 , -1.1316243 ,  1.2829843 ,  0.6779355 ,\n",
      "         -0.32166547, -0.51734763],\n",
      "        [ 0.48438653,...48 ],\n",
      "        [-0.8919766 , -0.5311779 ,  1.3061328 ,  0.70647854,\n",
      "          0.99664706, -1.344323  ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:466: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628...0.8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.173262...8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7f2231e45460>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.173262...8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7f2231e45460>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231e45820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231e45820>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7f2231e45460>\n",
      "a = NDArray([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628   1.5...965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       array_api.ewise_tanh(a)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628   1.5...965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]], device=cuda())\n",
      "self       = <needle.ops.Tanh object at 0x7f2231e45460>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:453: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[-1.5997132]]])\n",
      "_A         = array([[[-1.5997132]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7f22e30e70d0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:466: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-1.5997132]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7f2231de5af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7f2231de5af0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231de5a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231de5a90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7f2231de5af0>\n",
      "a = NDArray([[[-1.5997132]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       array_api.ewise_tanh(a)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.5997132]]], device=cpu())\n",
      "self       = <needle.ops.Tanh object at 0x7f2231de5af0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:453: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119   ... -0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]])\n",
      "_A         = array([[[ 1.6233783 , -0.58456963,  0.86539763,  0.8150671 ,\n",
      "          0.52470696, -2.4929152 ],\n",
      "        [ 0.28299394,...27 ],\n",
      "        [ 0.07837614, -0.39979827, -0.04655599, -1.3993185 ,\n",
      "          2.621195  , -0.13064201]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119  ...0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7f22e30e70d0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:466: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119   ... -0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119  ...0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7f2231cd1670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119  ...0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7f2231cd1670>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231cd12e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231cd12e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7f2231cd1670>\n",
      "a = NDArray([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119     -1.4...0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       array_api.ewise_tanh(a)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119     -1.4...0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]], device=cpu())\n",
      "self       = <needle.ops.Tanh object at 0x7f2231cd1670>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:453: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[-1.2775043]]])\n",
      "_A         = array([[[-1.2775043]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-1.2775043]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7f22e30e70d0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:466: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-1.2775043]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.2775043]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7f2231d0a940>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.2775043]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7f2231d0a940>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231d0a8e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231d0a8e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7f2231d0a940>\n",
      "a = NDArray([[[-1.2775043]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       array_api.ewise_tanh(a)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.2775043]]], device=cuda())\n",
      "self       = <needle.ops.Tanh object at 0x7f2231d0a940>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:453: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "_A         = array([[[-0.71145284,  0.96028835, -0.01700081,  0.18537076,\n",
      "         -0.92995864,  1.2871389 ],\n",
      "        [-0.69594264,...502],\n",
      "        [-0.03949696, -0.9310424 , -0.55855966, -1.2472047 ,\n",
      "         -1.1149031 , -0.5235176 ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.697433....02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7f22e30e70d0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:466: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.697433....02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7f2231da1ee0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.697433....02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7f2231da1ee0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231da1e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\") raised in repr()] Tensor object at 0x7f2231da1e50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7f2231da1ee0>\n",
      "a = NDArray([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334   0.7...3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       array_api.ewise_tanh(a)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'ewise_tanh'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334   0.7...3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]], device=cuda())\n",
      "self       = <needle.ops.Tanh object at 0x7f2231da1ee0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:453: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1...7868 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]])]\n",
      "A_t        = [tensor([[ 0.9080,  1.1915,  0.6277,  1.7709,  2.5803],\n",
      "        [ 1.0994, -0.2561, -0.3035, -1.2755,  1.4340],\n",
      "       ....9323],\n",
      "        [-0.1727, -0.3860,  0.6065,  0.3832, -1.2306],\n",
      "        [-0.7605,  1.3836, -1.3960, -0.2341,  0.9361]])]\n",
      "_A         = [array([[ 0.90795034,  1.1914672 ,  0.6276671 ,  1.7709465 ,  2.5802875 ],\n",
      "       [ 1.0994289 , -0.25607443, -0.303511...8323304, -1.2305677 ],\n",
      "       [-0.76054263,  1.3835862 , -1.396009  , -0.23408563,  0.9360824 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1...7868 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.2560...8 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231c8adf0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.2560...8 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231c8adf0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231c8adc0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231c8adc0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231c8adf0>\n",
      "args = (NDArray([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1.27553...6  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]], device=cpu()),)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1.27553...6  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]], device=cpu()),)\n",
      "self       = <needle.ops.Stack object at 0x7f2231c8adf0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0...071  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])]\n",
      "A_t        = [tensor([[ 0.2389,  1.7993,  0.1729, -0.1885,  1.6104],\n",
      "        [-0.5798, -1.7727,  0.7770, -0.6439,  0.3592],\n",
      "       ....3381],\n",
      "        [-1.3337, -0.5883, -1.3194, -2.0161,  0.7420],\n",
      "        [-1.2923,  0.1274, -1.2252,  1.4211,  1.0188]])]\n",
      "_A         = [array([[ 0.23885646,  1.7992946 ,  0.17290778, -0.18845753,  1.6103987 ],\n",
      "       [-0.57978904, -1.7727456 ,  0.776984...160983 ,  0.74200845],\n",
      "       [-1.2923242 ,  0.1273649 , -1.225224  ,  1.421074  ,  1.0188165 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0...071  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727...1  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231cca190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727...1  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231cca190>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231cca700>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231cca700>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231cca190>\n",
      "args = (NDArray([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0.64388...2  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]], device=cpu()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0.64388...2  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]], device=cpu()))\n",
      "self       = <needle.ops.Stack object at 0x7f2231cca190>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928 ... 1.0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])]\n",
      "A_t        = [tensor([[[-1.3506, -1.0219, -0.2898, -0.5111, -1.0806, -0.1566, -0.1900],\n",
      "         [-1.5699,  0.1507, -1.5068,  0.479...2785,  0.1121, -0.2383,  1.0998, -0.0714],\n",
      "         [ 0.6218,  0.6492,  1.0329, -0.8242, -0.0909,  0.3844,  0.3634]]])]\n",
      "_A         = [array([[[-1.3505836 , -1.0219276 , -0.28979254, -0.5110697 ,\n",
      "         -1.0806233 , -0.156554  , -0.18995644],\n",
      "       ...[ 0.62180406,  0.64916927,  1.0328673 , -0.82423985,\n",
      "         -0.09091089,  0.3844313 ,  0.36341754]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928 ... 1.0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.1899....0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231d21e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.1899....0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231d21e50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231d217c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231d217c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231d21e50>\n",
      "args = (NDArray([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928    0.1....07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]], device=cpu()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928    0.1....07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]], device=cpu()))\n",
      "self       = <needle.ops.Stack object at 0x7f2231d21e50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1...948   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]])]\n",
      "A_t        = [tensor([[-0.8576, -0.0482,  0.2440,  1.8527,  0.0825],\n",
      "        [-1.3493,  0.7654, -1.0452,  1.2154, -1.8788],\n",
      "       ....7364],\n",
      "        [ 0.6130,  0.6366, -0.1788,  1.4526,  0.0686],\n",
      "        [-0.0181, -0.1486, -1.0339, -1.0570, -0.2119]])]\n",
      "_A         = [array([[-0.85764205, -0.04821775,  0.24398687,  1.8527063 ,  0.08247776],\n",
      "       [-1.3493153 ,  0.7653674 , -1.045237...526075 ,  0.06863618],\n",
      "       [-0.0180711 , -0.14859328, -1.0338513 , -1.0570263 , -0.2118788 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1...948   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653...   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231e02490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653...   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231e02490>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231e024f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231e024f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231e02490>\n",
      "args = (NDArray([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1.21536... -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]], device=cuda()),)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1.21536... -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]], device=cuda()),)\n",
      "self       = <needle.ops.Stack object at 0x7f2231e02490>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0...7274  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])]\n",
      "A_t        = [tensor([[-0.0361,  1.2204,  0.7632, -0.0202,  2.2113],\n",
      "        [-0.4971,  1.6844, -0.6964,  0.4664,  0.3781],\n",
      "       ....2980],\n",
      "        [ 0.0499,  1.2440, -1.3311,  0.1839, -0.7144],\n",
      "        [-1.1293, -1.0743,  0.6784,  0.4646,  1.6899]])]\n",
      "_A         = [array([[-0.03613054,  1.2204468 ,  0.7631518 , -0.02020102,  2.2112603 ],\n",
      "       [-0.49705917,  1.6843735 , -0.696395...8387789, -0.7143793 ],\n",
      "       [-1.1292638 , -1.0743438 ,  0.6784423 ,  0.46458447,  1.6899037 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0...7274  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843...74  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231d60100>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843...74  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231d60100>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231d60b20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231d60b20>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231d60100>\n",
      "args = (NDArray([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0.46635...  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]], device=cuda()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0.46635...  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]], device=cuda()))\n",
      "self       = <needle.ops.Stack object at 0x7f2231d60100>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  ...02]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])]\n",
      "A_t        = [tensor([[[-5.5682e-01, -8.2674e-01, -9.5333e-01, -1.0309e-01, -4.2893e-01,\n",
      "           5.5746e-04,  1.0586e+00],\n",
      "     ...02],\n",
      "         [ 1.1807e+00, -5.7011e-01, -1.6425e-01,  1.0571e-01, -1.1592e-02,\n",
      "           6.3662e-02,  3.1759e-01]]])]\n",
      "_A         = [array([[[-5.56821287e-01, -8.26741934e-01, -9.53326285e-01,\n",
      "         -1.03087865e-01, -4.28928941e-01,  5.57456922e-0...011104e-01, -1.6424966e-01,  1.0570560e-01,\n",
      "         -1.1592215e-02,  6.3661553e-02,  3.1758705e-01]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  ...02]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-0...]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231d3bbe0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-0...]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231d3bbe0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231d3b0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231d3b0d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231d3bbe0>\n",
      "args = (NDArray([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  1.0586...53e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]], device=cuda()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  1.0586...53e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]], device=cuda()))\n",
      "self       = <needle.ops.Stack object at 0x7f2231d3bbe0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape0-0-1] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0...1785 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]])]\n",
      "A_t        = [tensor([[-0.9027, -0.7916,  2.7447,  1.3618, -0.8795],\n",
      "        [-0.8366, -1.3726,  0.8278, -0.1998, -1.3305],\n",
      "       ...0961, -0.1999, -0.4484,  1.0540,  0.0305],\n",
      "        [-0.4769,  1.5635, -1.0746,  0.9731, -1.0777]], requires_grad=True)]\n",
      "_A         = [array([[-0.9026891 , -0.7916189 ,  2.7447178 ,  1.3617536 , -0.8795233 ],\n",
      "       [-0.8365751 , -1.3725866 ,  0.827849...540462 ,  0.03045906],\n",
      "       [-0.4768823 ,  1.5634812 , -1.0746397 ,  0.9731278 , -1.0777118 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0...1785 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725...5 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231db8cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725...5 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231db8cd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231db8340>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231db8340>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231db8cd0>\n",
      "args = (NDArray([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0.19975...  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]], device=cpu()),)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0.19975...  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]], device=cpu()),)\n",
      "self       = <needle.ops.Stack object at 0x7f2231db8cd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape1-0-2] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0...7174  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])]\n",
      "A_t        = [tensor([[-1.1209, -0.4109, -0.1834,  1.0602,  0.6528],\n",
      "        [ 0.5972,  0.9693,  1.1120, -0.5864,  0.0547],\n",
      "       ...3290,  0.2979,  0.3833,  0.1805,  0.1305],\n",
      "        [ 0.8502,  0.6713,  1.1949, -0.3069, -0.4117]], requires_grad=True)]\n",
      "_A         = [array([[-1.1208949 , -0.41087958, -0.18338212,  1.0601658 ,  0.6527857 ],\n",
      "       [ 0.5972073 ,  0.9692809 ,  1.112041...8051435,  0.1305337 ],\n",
      "       [ 0.8501626 ,  0.6712771 ,  1.1948769 , -0.30687174, -0.41171578]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0...7174  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692...74  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231cd1f70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692...74  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231cd1f70>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231cd1b50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231cd1b50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231cd1f70>\n",
      "args = (NDArray([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0.58642...07  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]], device=cpu()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0.58642...07  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]], device=cpu()))\n",
      "self       = <needle.ops.Stack object at 0x7f2231cd1f70>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape2-2-5] ______________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.2332015...-1.0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])]\n",
      "A_t        = [tensor([[[-0.2897,  0.5881,  1.2632,  1.8495, -1.3362, -0.2293,  1.6220],\n",
      "         [ 0.2332,  0.4612, -1.1698,  1.140...0655, -0.2649],\n",
      "         [-2.1368, -0.2813,  0.5420, -0.8280, -0.1952,  1.2885,  1.6259]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[-0.28966966,  0.5881318 ,  1.2632097 ,  1.8494787 ,\n",
      "         -1.3362346 , -0.22933096,  1.6220489 ],\n",
      "       ...[-2.136809  , -0.28127787,  0.54197794, -0.8279596 ,\n",
      "         -0.19517688,  1.2884692 ,  1.625865  ]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.2332015...-1.0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220....0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231ddd070>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220....0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231ddd070>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231dddeb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231dddeb0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231ddd070>\n",
      "args = (NDArray([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.23320156  0.4....26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]], device=cpu()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.23320156  0.4....26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]], device=cpu()))\n",
      "self       = <needle.ops.Stack object at 0x7f2231ddd070>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape0-0-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0...197  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]])]\n",
      "A_t        = [tensor([[-1.3193,  0.6103, -1.2339, -0.6026,  0.8272],\n",
      "        [ 1.3493,  0.1192, -0.3294,  0.6092,  1.7408],\n",
      "       ...5734, -0.6012, -0.2230,  0.0861,  0.1369],\n",
      "        [ 1.5095, -0.7127, -0.7338, -1.1747, -0.3218]], requires_grad=True)]\n",
      "_A         = [array([[-1.3193388 ,  0.6103235 , -1.2338529 , -0.6025919 ,  0.82718664],\n",
      "       [ 1.349268  ,  0.11915252, -0.329409...8610313,  0.13694113],\n",
      "       [ 1.5095038 , -0.7126971 , -0.73380625, -1.1747231 , -0.3217507 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0...197  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.1191...  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231de2ca0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.1191...  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231de2ca0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231de2af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231de2af0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231de2ca0>\n",
      "args = (NDArray([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0.60915... -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]], device=cuda()),)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0.60915... -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]], device=cuda()),)\n",
      "self       = <needle.ops.Stack object at 0x7f2231de2ca0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape1-0-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2...63    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])]\n",
      "A_t        = [tensor([[ 0.8628, -1.1587, -1.0618,  0.0683,  0.4480],\n",
      "        [-1.6560,  0.3594,  0.5834, -2.7582,  0.1567],\n",
      "       ...2584,  0.9400,  1.4049,  2.0662,  1.9755],\n",
      "        [ 0.7066, -1.1856, -1.0203, -0.1288, -0.1320]], requires_grad=True)]\n",
      "_A         = [array([[ 0.86278856, -1.1587424 , -1.0617669 ,  0.06834911,  0.44795617],\n",
      "       [-1.6559892 ,  0.35942334,  0.583443...662467 ,  1.975455  ],\n",
      "       [ 0.70656043, -1.1856185 , -1.0202792 , -0.12881304, -0.13199753]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2...63    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.3594...    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231db89d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.3594...    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231db89d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231db8a60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231db8a60>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231db89d0>\n",
      "args = (NDArray([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2.75821...4  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]], device=cuda()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2.75821...4  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]], device=cuda()))\n",
      "self       = <needle.ops.Stack object at 0x7f2231db89d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape2-2-5] _____________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232 ...    0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])]\n",
      "A_t        = [tensor([[[-0.5643,  0.2627, -0.2986,  2.1484,  2.2822,  2.0546, -0.5192],\n",
      "         [ 0.5852,  0.2980, -0.4843, -0.240...2493, -1.1226],\n",
      "         [ 1.3219, -0.1445, -0.9866,  0.2616,  0.9347, -0.3751,  0.2546]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[-0.56430817,  0.26270753, -0.29860306,  2.1484196 ,\n",
      "          2.282201  ,  2.0546498 , -0.51918304],\n",
      "       ...[ 1.3219295 , -0.14445436, -0.9865747 ,  0.2616311 ,\n",
      "          0.93468   , -0.37505797,  0.25460657]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:492: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232 ...    0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918...  0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7f2231e18700>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918...  0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7f2231e18700>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231e18be0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f2231e18be0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7f2231e18700>\n",
      "args = (NDArray([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232   0.29...-1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]], device=cuda()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232   0.29...-1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]], device=cuda()))\n",
      "self       = <needle.ops.Stack object at 0x7f2231e18700>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:481: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______________________ test_summation[cpu-shape0-None] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.284491]]])\n",
      "_A         = array([[[0.284491]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[0.284491]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.284491]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231e56280>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.284491]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231e56280>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231e56c10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231e56c10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231e56280>\n",
      "a = NDArray([[[0.284491]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.284491]]], device=cpu())\n",
      "self       = <needle.ops.Summation object at 0x7f2231e56280>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_summation[cpu-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]])\n",
      "_A         = array([[ 0.7043908 ,  1.2221886 ,  1.5080935 ],\n",
      "       [-0.585842  ,  0.04551945,  1.1883969 ],\n",
      "       [ 0.35897374,  ...4381 ],\n",
      "       [-0.14003062, -0.20621192, -0.23054962],\n",
      "       [-2.1563048 , -1.2538761 ,  1.090643  ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231cc3790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231cc3790>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231cc34c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231cc34c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231cc3790>\n",
      "a = NDArray([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]], device=cpu())\n",
      "self       = <needle.ops.Summation object at 0x7f2231cc3790>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_summation[cpu-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0....4]\n",
      "  [-2.0539176  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]])\n",
      "_A         = array([[[-0.51554435, -1.0567564 ],\n",
      "        [-0.30265552,  0.4281822 ],\n",
      "        [-0.68831074,  1.6516227 ]],\n",
      "\n",
      "       [...  [[ 0.68060917,  0.5635482 ],\n",
      "        [ 1.0085729 ,  0.75971615],\n",
      "        [ 0.31350297, -2.0439312 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0....4]\n",
      "  [-2.0539176  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0...\n",
      "  [-2.0539176  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231e2a160>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0...\n",
      "  [-2.0539176  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231e2a160>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231e2a9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231e2a9d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231e2a160>\n",
      "a = NDArray([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0.276385...76  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0.276385...76  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]], device=cpu())\n",
      "self       = <needle.ops.Summation object at 0x7f2231e2a160>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_summation[cpu-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0....3]\n",
      "  [ 0.06753551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]])\n",
      "_A         = array([[[-0.09097444,  0.11011965],\n",
      "        [-0.23238643, -1.2567368 ],\n",
      "        [-0.00447594,  0.03316377]],\n",
      "\n",
      "       [...  [[ 0.4651775 , -0.96335906],\n",
      "        [-0.22114491,  1.0182978 ],\n",
      "        [ 0.30359092, -1.609142  ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0....3]\n",
      "  [ 0.06753551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0...\n",
      "  [ 0.06753551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231de42e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0...\n",
      "  [ 0.06753551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231de42e0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231de4ac0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231de4ac0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231de42e0>\n",
      "a = NDArray([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0.131353...551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0.131353...551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]], device=cpu())\n",
      "self       = <needle.ops.Summation object at 0x7f2231de42e0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_summation[cuda-shape0-None] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.87107515]]])\n",
      "_A         = array([[[0.87107515]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[0.87107515]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.87107515]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231ddb700>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.87107515]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231ddb700>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231ddb340>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231ddb340>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231ddb700>\n",
      "a = NDArray([[[0.87107515]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.87107515]]], device=cuda())\n",
      "self       = <needle.ops.Summation object at 0x7f2231ddb700>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]])\n",
      "_A         = array([[ 0.19680066, -0.31015334,  0.9066847 ],\n",
      "       [-0.15136234,  0.8248054 ,  0.588381  ],\n",
      "       [ 1.102897  ,  ...67875],\n",
      "       [ 0.49103832,  0.46082434,  0.2423399 ],\n",
      "       [ 2.015139  ,  0.10407976,  0.60018444]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231d5c3a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231d5c3a0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231d5c8b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231d5c8b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231d5c3a0>\n",
      "a = NDArray([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]], device=cuda())\n",
      "self       = <needle.ops.Summation object at 0x7f2231d5c3a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.... ]\n",
      "  [-1.0777732   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]])\n",
      "_A         = array([[[-0.3742769 , -0.15478124],\n",
      "        [-0.5490968 ,  0.1884813 ],\n",
      "        [-0.6726331 , -0.5823771 ]],\n",
      "\n",
      "       [...  [[ 0.72896844, -1.4150094 ],\n",
      "        [ 0.39689642, -2.1479504 ],\n",
      "        [ 0.84768903, -0.5966049 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.... ]\n",
      "  [-1.0777732   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0...\n",
      "  [-1.0777732   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231ddb580>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0...\n",
      "  [-1.0777732   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231ddb580>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231ddb7c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231ddb7c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231ddb580>\n",
      "a = NDArray([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.516130...2   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.516130...2   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]], device=cuda())\n",
      "self       = <needle.ops.Summation object at 0x7f2231ddb580>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1....4]\n",
      "  [ 0.40885907  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]])\n",
      "_A         = array([[[-0.43343157, -0.5803195 ],\n",
      "        [-0.79609615,  0.6915893 ],\n",
      "        [ 0.4549692 ,  0.6378628 ]],\n",
      "\n",
      "       [...  [[ 1.8298346 , -0.45180562],\n",
      "        [-0.16761442,  0.9780772 ],\n",
      "        [ 0.9505592 ,  0.0114131 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1....4]\n",
      "  [ 0.40885907  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1...\n",
      "  [ 0.40885907  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231c8d640>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1...\n",
      "  [ 0.40885907  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231c8d640>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231c8d040>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231c8d040>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231c8d640>\n",
      "a = NDArray([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1.329009...07  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1.329009...07  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]], device=cuda())\n",
      "self       = <needle.ops.Summation object at 0x7f2231c8d640>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m___________________ test_summation_backward[cpu-shape0-None] ___________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[0.25179562]]])\n",
      "_A         = array([[[0.25179562]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[0.25179562]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f22e30e53a0>\n",
      "        kwargs     = {'axes': None}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[0.25179562]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.25179562]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231d2ecd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.25179562]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231d2ecd0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231d2eeb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231d2eeb0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231d2ecd0>\n",
      "a = NDArray([[[0.25179562]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.25179562]]], device=cpu())\n",
      "self       = <needle.ops.Summation object at 0x7f2231d2ecd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape1-0] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]])\n",
      "_A         = array([[-0.26194048,  0.36832854,  0.524743  ],\n",
      "       [ 0.72734344, -0.6690794 ,  0.2118702 ],\n",
      "       [-0.17624503, -...3194 ],\n",
      "       [ 0.21689712,  0.43637753, -0.49504337],\n",
      "       [ 1.4166641 , -0.2632424 ,  1.5031598 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f22e30e53a0>\n",
      "        kwargs     = {'axes': 0}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231c8da00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231c8da00>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231c8d4c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231c8d4c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231c8da00>\n",
      "a = NDArray([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]], device=cpu())\n",
      "self       = <needle.ops.Summation object at 0x7f2231c8da00>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape2-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1....7]\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]])\n",
      "_A         = array([[[-1.2881242 , -0.24279231],\n",
      "        [ 0.5133875 ,  1.1767663 ],\n",
      "        [-1.223357  ,  1.9838535 ]],\n",
      "\n",
      "       [...  [[-0.17416234,  0.21612613],\n",
      "        [ 0.66463155,  0.21836282],\n",
      "        [ 0.6723505 , -0.5805451 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1...\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f22e30e53a0>\n",
      "        kwargs     = {'axes': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1....7]\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1...\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231caf3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1...\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231caf3d0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231caf7f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231caf7f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231caf3d0>\n",
      "a = NDArray([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1.587832...96  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1.587832...96  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]], device=cpu())\n",
      "self       = <needle.ops.Summation object at 0x7f2231caf3d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape3-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.... ]\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]])\n",
      "_A         = array([[[ 0.94992834, -0.13286018],\n",
      "        [-0.05775288,  2.3362038 ],\n",
      "        [-0.02063232,  0.17829332]],\n",
      "\n",
      "       [...  [[-0.04282787,  1.6461416 ],\n",
      "        [ 0.21175696, -0.98908705],\n",
      "        [-0.79134655,  1.2193956 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1...\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f22e30e53a0>\n",
      "        kwargs     = {'axes': 2}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.... ]\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1...\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f22e3860ca0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1...\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f22e3860ca0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f22e3860640>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f22e3860640>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f22e3860ca0>\n",
      "a = NDArray([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755...32  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755...32  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]], device=cpu())\n",
      "self       = <needle.ops.Summation object at 0x7f22e3860ca0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m__________________ test_summation_backward[cuda-shape0-None] ___________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[1.2554915]]])\n",
      "_A         = array([[[1.2554915]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[1.2554915]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f22e30e53a0>\n",
      "        kwargs     = {'axes': None}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[1.2554915]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[1.2554915]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f22e387f0a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[1.2554915]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f22e387f0a0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f22e387f040>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f22e387f040>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f22e387f0a0>\n",
      "a = NDArray([[[1.2554915]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[1.2554915]]], device=cuda())\n",
      "self       = <needle.ops.Summation object at 0x7f22e387f0a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape1-0] ____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]])\n",
      "_A         = array([[-0.01285293,  0.494915  , -0.7648952 ],\n",
      "       [-0.33858636,  1.2390388 , -1.1854753 ],\n",
      "       [ 0.45743605,  ...21282],\n",
      "       [ 1.6258224 ,  0.65709186,  0.06870755],\n",
      "       [-1.2057985 , -1.0558244 ,  1.0220021 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f22e30e53a0>\n",
      "        kwargs     = {'axes': 0}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f2231ddb490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f2231ddb490>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231ddb340>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231ddb340>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f2231ddb490>\n",
      "a = NDArray([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]], device=cuda())\n",
      "self       = <needle.ops.Summation object at 0x7f2231ddb490>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape2-1] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.... ]\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]])\n",
      "_A         = array([[[-0.20333268,  0.9668824 ],\n",
      "        [-0.25639066,  0.5157999 ],\n",
      "        [-0.44359782,  0.83550507]],\n",
      "\n",
      "       [...  [[ 0.3785013 , -1.1597295 ],\n",
      "        [ 0.8766111 , -1.7572289 ],\n",
      "        [ 0.31880176, -1.6526436 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1...\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f22e30e53a0>\n",
      "        kwargs     = {'axes': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.... ]\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1...\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f22e3858460>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1...\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f22e3858460>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f22e3858190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f22e3858190>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f22e3858460>\n",
      "a = NDArray([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.349219...94  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.349219...94  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]], device=cuda())\n",
      "self       = <needle.ops.Summation object at 0x7f22e3858460>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape3-2] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      "...2054880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]])\n",
      "_A         = array([[[ 3.1887993e-01,  1.5159522e-03],\n",
      "        [-9.1810602e-01,  8.5386848e-01],\n",
      "        [ 5.3351974e-01,  6.397005...,  3.2395300e-01],\n",
      "        [ 2.3655061e-01,  2.1437683e+00],\n",
      "        [ 1.1814048e+00, -1.3010720e+00]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "...54880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f22e30e53a0>\n",
      "        kwargs     = {'axes': 2}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:298: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      "...2054880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "...54880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7f22e38673a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "...54880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7f22e38673a0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f22e3867730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f22e3867730>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7f22e38673a0>\n",
      "a = NDArray([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      " [[-3.... [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.sum(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      " [[-3.... [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]], device=cuda())\n",
      "self       = <needle.ops.Summation object at 0x7f22e38673a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:279: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes0-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.76478755]]])\n",
      "_A         = array([[[-0.76478755]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.76478755]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.76478755]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f22e3865340>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.76478755]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f22e3865340>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f22e38655e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f22e38655e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f22e3865340>\n",
      "a = NDArray([[[-0.76478755]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.76478755]]], device=cpu())\n",
      "order      = [1, 0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7f22e3865340>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:213: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes0-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399 ...1.4833229   0.02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]])\n",
      "_A         = array([[[ 1.428199  , -0.42389697,  0.8514999 ,  1.2843655 ,\n",
      "          0.4156105 ,  2.2930398 ],\n",
      "        [-0.02262507,...98 ],\n",
      "        [-0.88521427,  0.6966768 , -0.3164875 , -0.19568315,\n",
      "          0.7862357 , -0.92159903]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399 ...1.4833229   0.02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399...4833229   0.02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231e02a30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399...4833229   0.02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231e02a30>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e02c10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e02c10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231e02a30>\n",
      "a = NDArray([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399  -0.08...02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399  -0.08...02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]], device=cpu())\n",
      "order      = [1, 0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231e02a30>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:213: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes1-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 2), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.5497496]]])\n",
      "_A         = array([[[0.5497496]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[0.5497496]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.5497496]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231e3ee50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.5497496]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231e3ee50>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e3ef70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e3ef70>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231e3ee50>\n",
      "a = NDArray([[[0.5497496]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.5497496]]], device=cpu())\n",
      "order      = [2, 1, 0]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231e3ee50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:213: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes1-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 2), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.9221655...0.5732212   1.6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]])\n",
      "_A         = array([[[ 1.1175423 ,  0.2273078 ,  0.01652352,  0.6953092 ,\n",
      "          0.37275058, -0.1833131 ],\n",
      "        [ 0.5189381 ,...12 ],\n",
      "        [-0.5111894 ,  0.16540636,  1.0318601 , -0.7140344 ,\n",
      "          0.55253035,  0.5942802 ]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.9221655...0.5732212   1.6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.922165...5732212   1.6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231e56fa0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.922165...5732212   1.6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231e56fa0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e56f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e56f10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231e56fa0>\n",
      "a = NDArray([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.9221655  -0.6...6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.9221655  -0.6...6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]], device=cpu())\n",
      "order      = [2, 1, 0]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231e56fa0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:213: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.252342]]])\n",
      "_A         = array([[[0.252342]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[0.252342]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.252342]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231cd1bb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.252342]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231cd1bb0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231cd1be0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231cd1be0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231cd1bb0>\n",
      "a = NDArray([[[0.252342]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      "            \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            order = order[ :-\u001b[94m2\u001b[39;49;00m] + [order[-\u001b[94m1\u001b[39;49;00m], order[-\u001b[94m2\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.252342]]], device=cpu())\n",
      "order      = [0, 2, 1]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231cd1bb0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:216: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.03860308...0.18599093  0.26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]])\n",
      "_A         = array([[[-0.134239  ,  0.75096023, -1.4823366 , -0.40712276,\n",
      "          0.7533559 , -1.0401373 ],\n",
      "        [-0.839335  ,...27 ],\n",
      "        [ 1.20037   ,  1.4225695 ,  0.14660548, -0.4510802 ,\n",
      "          0.5955475 , -0.59707063]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.03860308...0.18599093  0.26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.0386030...18599093  0.26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231d69af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.0386030...18599093  0.26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231d69af0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231d69550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231d69550>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231d69af0>\n",
      "a = NDArray([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.03860308 -0.65...26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      "            \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            order = order[ :-\u001b[94m2\u001b[39;49;00m] + [order[-\u001b[94m1\u001b[39;49;00m], order[-\u001b[94m2\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.03860308 -0.65...26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]], device=cpu())\n",
      "order      = [0, 2, 1]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231d69af0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:216: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[2.0202532]]])\n",
      "_A         = array([[[2.0202532]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[2.0202532]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[2.0202532]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231dc94c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[2.0202532]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231dc94c0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231dc96a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231dc96a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231dc94c0>\n",
      "a = NDArray([[[2.0202532]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[2.0202532]]], device=cuda())\n",
      "order      = [1, 0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231dc94c0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:213: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517 ....92294955  0.19945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]])\n",
      "_A         = array([[[ 0.43215325,  0.45116025, -0.22739509,  1.115375  ,\n",
      "         -1.4533465 ,  0.12058777],\n",
      "        [ 0.01037335,...46 ],\n",
      "        [ 0.3929003 ,  0.03355066, -0.27658293,  1.1708506 ,\n",
      "         -0.11228849, -0.09884176]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517 ....92294955  0.19945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517...2294955  0.19945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231cdfb50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517...2294955  0.19945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231cdfb50>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231cdfa60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231cdfa60>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231cdfb50>\n",
      "a = NDArray([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517  -0.63...945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517  -0.63...945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]], device=cuda())\n",
      "order      = [1, 0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231cdfb50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:213: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 2), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.14226767]]])\n",
      "_A         = array([[[0.14226767]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[0.14226767]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.14226767]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231e4bd00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.14226767]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231e4bd00>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e4b5b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e4b5b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231e4bd00>\n",
      "a = NDArray([[[0.14226767]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.14226767]]], device=cuda())\n",
      "order      = [2, 1, 0]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231e4bd00>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:213: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 2), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.04896045...-0.3429354  -0.3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]])\n",
      "_A         = array([[[-0.45924038, -1.0967047 , -0.7741067 , -1.6878831 ,\n",
      "          0.7713184 , -0.17223054],\n",
      "        [-0.6361363 ,...345],\n",
      "        [-0.15298192,  0.0542486 ,  0.12609395, -0.8680353 ,\n",
      "         -1.1219878 ,  0.32406053]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.04896045...-0.3429354  -0.3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.0489604....3429354  -0.3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231e2a640>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.0489604....3429354  -0.3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231e2a640>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e2a4f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231e2a4f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231e2a640>\n",
      "a = NDArray([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.04896045 -0.54...3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.04896045 -0.54...3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]], device=cuda())\n",
      "order      = [2, 1, 0]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231e2a640>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:213: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.33231008]]])\n",
      "_A         = array([[[-0.33231008]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.33231008]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.33231008]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f22e3828dc0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.33231008]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f22e3828dc0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f22e3828b50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f22e3828b50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f22e3828dc0>\n",
      "a = NDArray([[[-0.33231008]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      "            \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            order = order[ :-\u001b[94m2\u001b[39;49;00m] + [order[-\u001b[94m1\u001b[39;49;00m], order[-\u001b[94m2\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.33231008]]], device=cuda())\n",
      "order      = [0, 2, 1]\n",
      "self       = <needle.ops.Transpose object at 0x7f22e3828dc0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:216: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.5728882... -1.5549369  -1.361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]])\n",
      "_A         = array([[[-0.80980945, -0.58157414,  1.3430595 ,  0.14294803,\n",
      "         -0.43587744, -0.8963174 ],\n",
      "        [ 0.12310173,...281],\n",
      "        [-0.729695  , -0.27707356,  2.2894404 ,  1.4521582 ,\n",
      "         -1.4271829 ,  0.22722833]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:226: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.5728882... -1.5549369  -1.361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.572888...1.5549369  -1.361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7f2231cf4790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.572888...1.5549369  -1.361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7f2231cf4790>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231cf4730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7f2231cf4730>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7f2231cf4790>\n",
      "a = NDArray([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.5728882   0.1....361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[90m# `Transpose` behaves differently from `numpy.transpose`\u001b[39;49;00m\n",
      "        \u001b[90m# in terms of input and default axes permutated.\u001b[39;49;00m\n",
      "        order = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(a.shape)))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]], order[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]]\n",
      "            \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            order = order[ :-\u001b[94m2\u001b[39;49;00m] + [order[-\u001b[94m1\u001b[39;49;00m], order[-\u001b[94m2\u001b[39;49;00m]]\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=order)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.5728882   0.1....361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]], device=cuda())\n",
      "order      = [0, 2, 1]\n",
      "self       = <needle.ops.Transpose object at 0x7f2231cf4790>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:216: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_logsumexp[cpu-shape0-None] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.5344331]]])\n",
      "A_t        = tensor([[[-1.5344]]])\n",
      "_A         = array([[[-1.5344331]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "t_axes     = (0, 1, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:447: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-1.5344331]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.5344331]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7f2231ddd400>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.5344331]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7f2231ddd400>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231ddd730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231ddd730>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7f2231ddd400>\n",
      "Z = NDArray([[[-1.5344331]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.max = Z.max(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            \u001b[90m# broadcast `z_max`` to shape of `Z`\u001b[39;49;00m\n",
      "            reduced_shape = \u001b[96mlist\u001b[39;49;00m(Z.shape)\n",
      "            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "                reduced_shape[i] = \u001b[94m1\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.reduced_shape = \u001b[96mtuple\u001b[39;49;00m(reduced_shape)\n",
      "    \n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.max + \\\n",
      "                   array_api.log(array_api.sum(array_api.exp(Z -\n",
      "                                                             array_api.broadcast_to(array_api.reshape(\u001b[96mself\u001b[39;49;00m.max,\n",
      "                                                                                                      \u001b[96mself\u001b[39;49;00m.reduced_shape),\n",
      "                                                                                    Z.shape)),\n",
      "                                               \u001b[96mself\u001b[39;49;00m.axes))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            \u001b[96mself\u001b[39;49;00m.reduced_shape = (\u001b[94m1\u001b[39;49;00m,) * \u001b[96mlen\u001b[39;49;00m(Z.shape)\n",
      ">           \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.max + array_api.log(array_api.sum(array_api.exp(Z - \u001b[96mself\u001b[39;49;00m.max)))\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-1.5344331]]], device=cpu())\n",
      "self       = <needle.ops.LogSumExp object at 0x7f2231ddd400>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:427: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]])\n",
      "A_t        = tensor([[ 0.0536,  0.7091, -0.6595],\n",
      "        [ 0.4210,  0.7163, -0.5331],\n",
      "        [ 0.7600, -0.5297,  0.1033],\n",
      "        [ 0.6210,  0.8935, -0.1603],\n",
      "        [-0.2013, -0.2559,  0.7845]])\n",
      "_A         = array([[ 0.05357093,  0.7091037 , -0.6595013 ],\n",
      "       [ 0.4209701 ,  0.7163161 , -0.53314596],\n",
      "       [ 0.75999004, -...2609 ],\n",
      "       [ 0.62098956,  0.8935139 , -0.16026992],\n",
      "       [-0.20125377, -0.25592747,  0.7845221 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:447: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7f2231de28b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7f2231de28b0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231de2310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231de2310>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7f2231de28b0>\n",
      "Z = NDArray([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.max = Z.max(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            \u001b[90m# broadcast `z_max`` to shape of `Z`\u001b[39;49;00m\n",
      "            reduced_shape = \u001b[96mlist\u001b[39;49;00m(Z.shape)\n",
      "            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "                reduced_shape[i] = \u001b[94m1\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.reduced_shape = \u001b[96mtuple\u001b[39;49;00m(reduced_shape)\n",
      "    \n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.max + \\\n",
      "                   array_api.log(array_api.sum(array_api.exp(Z -\n",
      "                                                             array_api.broadcast_to(array_api.reshape(\u001b[96mself\u001b[39;49;00m.max,\n",
      "                                                                                                      \u001b[96mself\u001b[39;49;00m.reduced_shape),\n",
      "                                                                                    Z.shape)),\n",
      "                                               \u001b[96mself\u001b[39;49;00m.axes))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            \u001b[96mself\u001b[39;49;00m.reduced_shape = (\u001b[94m1\u001b[39;49;00m,) * \u001b[96mlen\u001b[39;49;00m(Z.shape)\n",
      ">           \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.max + array_api.log(array_api.sum(array_api.exp(Z - \u001b[96mself\u001b[39;49;00m.max)))\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]], device=cpu())\n",
      "self       = <needle.ops.LogSumExp object at 0x7f2231de28b0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:427: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0....3]\n",
      "  [-1.0486687   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]])\n",
      "A_t        = tensor([[[-0.1867,  0.0383],\n",
      "         [-0.0550, -0.3202],\n",
      "         [-0.9666, -2.5009]],\n",
      "\n",
      "        [[-0.0104,  0.2827],\n",
      "...         [-1.0487,  0.9679]],\n",
      "\n",
      "        [[ 1.3501,  0.8159],\n",
      "         [-0.7212,  0.4590],\n",
      "         [ 0.2047,  2.6674]]])\n",
      "_A         = array([[[-0.18668747,  0.03825841],\n",
      "        [-0.05503474, -0.3201902 ],\n",
      "        [-0.966571  , -2.500891  ]],\n",
      "\n",
      "       [...  [[ 1.3500627 ,  0.8158523 ],\n",
      "        [-0.72117835,  0.45901817],\n",
      "        [ 0.20466456,  2.6674361 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:447: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0....3]\n",
      "  [-1.0486687   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0...\n",
      "  [-1.0486687   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7f2231caf640>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0...\n",
      "  [-1.0486687   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7f2231caf640>\n",
      "        tensor     = <[TypeError(\"'int' object is not iterable\") raised in repr()] Tensor object at 0x7f2231caff40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'int' object is not iterable\") raised in repr()] Tensor object at 0x7f2231caff40>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7f2231caf640>\n",
      "Z = NDArray([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0.282676...87   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.max = Z.max(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            \u001b[90m# broadcast `z_max`` to shape of `Z`\u001b[39;49;00m\n",
      "            reduced_shape = \u001b[96mlist\u001b[39;49;00m(Z.shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0.282676...87   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]], device=cpu())\n",
      "reduced_shape = [8, 3, 2]\n",
      "self       = <needle.ops.LogSumExp object at 0x7f2231caf640>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:415: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0.... ]\n",
      "  [-1.4058412  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]])\n",
      "A_t        = tensor([[[-1.7313,  2.3088],\n",
      "         [ 0.1194,  0.6174],\n",
      "         [-0.3792, -0.1933]],\n",
      "\n",
      "        [[-0.2145,  0.6835],\n",
      "...         [-1.4058, -1.3156]],\n",
      "\n",
      "        [[-0.8430, -0.4802],\n",
      "         [-0.4784,  0.9575],\n",
      "         [-1.0233, -1.1449]]])\n",
      "_A         = array([[[-1.7313014 ,  2.3088462 ],\n",
      "        [ 0.11944567,  0.61742485],\n",
      "        [-0.37917662, -0.1933147 ]],\n",
      "\n",
      "       [...  [[-0.8429701 , -0.48019046],\n",
      "        [-0.47839305,  0.95748687],\n",
      "        [-1.0232737 , -1.1449004 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:447: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0.... ]\n",
      "  [-1.4058412  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0...\n",
      "  [-1.4058412  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7f2231ce3520>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0...\n",
      "  [-1.4058412  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7f2231ce3520>\n",
      "        tensor     = <[TypeError(\"'int' object is not iterable\") raised in repr()] Tensor object at 0x7f2231ce3670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'int' object is not iterable\") raised in repr()] Tensor object at 0x7f2231ce3670>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7f2231ce3520>\n",
      "Z = NDArray([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0.683539...12  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.max = Z.max(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            \u001b[90m# broadcast `z_max`` to shape of `Z`\u001b[39;49;00m\n",
      "            reduced_shape = \u001b[96mlist\u001b[39;49;00m(Z.shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0.683539...12  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]], device=cpu())\n",
      "reduced_shape = [8, 3, 2]\n",
      "self       = <needle.ops.LogSumExp object at 0x7f2231ce3520>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:415: TypeError\n",
      "\u001b[31m\u001b[1m_______________________ test_logsumexp[cuda-shape0-None] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.833531]]])\n",
      "A_t        = tensor([[[-1.8335]]])\n",
      "_A         = array([[[-1.833531]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "t_axes     = (0, 1, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:447: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-1.833531]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.833531]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7f2231d5c700>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.833531]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7f2231d5c700>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231d5cd00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231d5cd00>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7f2231d5c700>\n",
      "Z = NDArray([[[-1.833531]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.max = Z.max(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            \u001b[90m# broadcast `z_max`` to shape of `Z`\u001b[39;49;00m\n",
      "            reduced_shape = \u001b[96mlist\u001b[39;49;00m(Z.shape)\n",
      "            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "                reduced_shape[i] = \u001b[94m1\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.reduced_shape = \u001b[96mtuple\u001b[39;49;00m(reduced_shape)\n",
      "    \n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.max + \\\n",
      "                   array_api.log(array_api.sum(array_api.exp(Z -\n",
      "                                                             array_api.broadcast_to(array_api.reshape(\u001b[96mself\u001b[39;49;00m.max,\n",
      "                                                                                                      \u001b[96mself\u001b[39;49;00m.reduced_shape),\n",
      "                                                                                    Z.shape)),\n",
      "                                               \u001b[96mself\u001b[39;49;00m.axes))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            \u001b[96mself\u001b[39;49;00m.reduced_shape = (\u001b[94m1\u001b[39;49;00m,) * \u001b[96mlen\u001b[39;49;00m(Z.shape)\n",
      ">           \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.max + array_api.log(array_api.sum(array_api.exp(Z - \u001b[96mself\u001b[39;49;00m.max)))\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-1.833531]]], device=cuda())\n",
      "self       = <needle.ops.LogSumExp object at 0x7f2231d5c700>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:427: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]])\n",
      "A_t        = tensor([[ 1.3200,  0.8519, -0.9509],\n",
      "        [-0.2586,  0.8289,  0.3701],\n",
      "        [-0.0037,  0.9473,  0.4353],\n",
      "        [-0.3471, -0.6862,  0.2598],\n",
      "        [ 0.7514, -1.2294, -1.8327]])\n",
      "_A         = array([[ 1.3200428 ,  0.8519144 , -0.9508507 ],\n",
      "       [-0.25861153,  0.828859  ,  0.37011066],\n",
      "       [-0.00371555,  ...29144],\n",
      "       [-0.34713647, -0.6862239 ,  0.25979483],\n",
      "       [ 0.75136185, -1.2294345 , -1.8326827 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:447: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7f2231e34be0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7f2231e34be0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231e34df0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'sum'\") raised in repr()] Tensor object at 0x7f2231e34df0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7f2231e34be0>\n",
      "Z = NDArray([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.max = Z.max(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            \u001b[90m# broadcast `z_max`` to shape of `Z`\u001b[39;49;00m\n",
      "            reduced_shape = \u001b[96mlist\u001b[39;49;00m(Z.shape)\n",
      "            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "                reduced_shape[i] = \u001b[94m1\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.reduced_shape = \u001b[96mtuple\u001b[39;49;00m(reduced_shape)\n",
      "    \n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.max + \\\n",
      "                   array_api.log(array_api.sum(array_api.exp(Z -\n",
      "                                                             array_api.broadcast_to(array_api.reshape(\u001b[96mself\u001b[39;49;00m.max,\n",
      "                                                                                                      \u001b[96mself\u001b[39;49;00m.reduced_shape),\n",
      "                                                                                    Z.shape)),\n",
      "                                               \u001b[96mself\u001b[39;49;00m.axes))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            \u001b[96mself\u001b[39;49;00m.reduced_shape = (\u001b[94m1\u001b[39;49;00m,) * \u001b[96mlen\u001b[39;49;00m(Z.shape)\n",
      ">           \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.max + array_api.log(array_api.sum(array_api.exp(Z - \u001b[96mself\u001b[39;49;00m.max)))\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'sum'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]], device=cuda())\n",
      "self       = <needle.ops.LogSumExp object at 0x7f2231e34be0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:427: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0.... ]\n",
      "  [-0.44889545  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]])\n",
      "A_t        = tensor([[[ 0.6772,  0.1087],\n",
      "         [-0.3206, -0.5328],\n",
      "         [ 1.4481,  2.3238]],\n",
      "\n",
      "        [[ 2.6118,  0.0611],\n",
      "...         [-0.4489,  0.2671]],\n",
      "\n",
      "        [[ 0.0928, -0.4465],\n",
      "         [-0.3253,  0.3682],\n",
      "         [ 0.6174,  0.3607]]])\n",
      "_A         = array([[[ 0.6771947 ,  0.10870803],\n",
      "        [-0.32060415, -0.5328463 ],\n",
      "        [ 1.4481194 ,  2.3237994 ]],\n",
      "\n",
      "       [...  [[ 0.09279188, -0.44650966],\n",
      "        [-0.32530743,  0.36815342],\n",
      "        [ 0.6174357 ,  0.36072657]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:447: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0.... ]\n",
      "  [-0.44889545  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0...\n",
      "  [-0.44889545  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7f2231d01b20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0...\n",
      "  [-0.44889545  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7f2231d01b20>\n",
      "        tensor     = <[TypeError(\"'int' object is not iterable\") raised in repr()] Tensor object at 0x7f2231d012b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'int' object is not iterable\") raised in repr()] Tensor object at 0x7f2231d012b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7f2231d01b20>\n",
      "Z = NDArray([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0.061128...45  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.max = Z.max(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            \u001b[90m# broadcast `z_max`` to shape of `Z`\u001b[39;49;00m\n",
      "            reduced_shape = \u001b[96mlist\u001b[39;49;00m(Z.shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0.061128...45  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]], device=cuda())\n",
      "reduced_shape = [8, 3, 2]\n",
      "self       = <needle.ops.LogSumExp object at 0x7f2231d01b20>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:415: TypeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0.... ]\n",
      "  [-0.90701824 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]])\n",
      "A_t        = tensor([[[-0.4410,  0.0718],\n",
      "         [-1.5023, -0.0198],\n",
      "         [-2.9242,  0.6088]],\n",
      "\n",
      "        [[ 0.6106,  0.8656],\n",
      "...         [-0.9070, -0.3669]],\n",
      "\n",
      "        [[ 0.3390, -0.8299],\n",
      "         [ 0.2380,  0.3032],\n",
      "         [-0.7334, -0.7552]]])\n",
      "_A         = array([[[-0.44101718,  0.07179229],\n",
      "        [-1.5022818 , -0.0197843 ],\n",
      "        [-2.9241614 ,  0.60881114]],\n",
      "\n",
      "       [...  [[ 0.33902806, -0.8299044 ],\n",
      "        [ 0.23796475,  0.3032223 ],\n",
      "        [-0.73337084, -0.7552029 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:447: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0.... ]\n",
      "  [-0.90701824 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0...\n",
      "  [-0.90701824 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7f22e3852430>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:209: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0...\n",
      "  [-0.90701824 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7f22e3852430>\n",
      "        tensor     = <[TypeError(\"'int' object is not iterable\") raised in repr()] Tensor object at 0x7f22e3852280>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'int' object is not iterable\") raised in repr()] Tensor object at 0x7f22e3852280>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7f22e3852430>\n",
      "Z = NDArray([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0.865607...24 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.max = Z.max(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            \u001b[90m# broadcast `z_max`` to shape of `Z`\u001b[39;49;00m\n",
      "            reduced_shape = \u001b[96mlist\u001b[39;49;00m(Z.shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0.865607...24 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]], device=cuda())\n",
      "reduced_shape = [8, 3, 2]\n",
      "self       = <needle.ops.LogSumExp object at 0x7f22e3852430>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:415: TypeError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] - Attribut...\n",
      "FAILED tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] - Attribut...\n",
      "FAILED tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] - Attribu...\n",
      "FAILED tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] - Attribu...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-16-16-16] - AttributeError: ...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-8-8-8] - AttributeError: mod...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-1-2-3] - AttributeError: mod...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-3-4-5] - AttributeError: mod...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-5-4-3] - AttributeError: mod...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-16-16-32] - AttributeError: ...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-64-64-64] - AttributeError: ...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-72-72-72] - AttributeError: ...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-72-73-74] - AttributeError: ...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-74-73-72] - AttributeError: ...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cpu-128-128-128] - AttributeErro...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-16-16-16] - AttributeError:...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-8-8-8] - AttributeError: mo...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-1-2-3] - AttributeError: mo...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-3-4-5] - AttributeError: mo...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-5-4-3] - AttributeError: mo...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-16-16-32] - AttributeError:...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-64-64-64] - AttributeError:...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-72-72-72] - AttributeError:...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-72-73-74] - AttributeError:...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-74-73-72] - AttributeError:...\n",
      "FAILED tests/test_nd_backend.py::test_matmul[cuda-128-128-128] - AttributeErr...\n",
      "FAILED tests/test_nd_backend.py::test_power[cpu-shape0] - AttributeError: mod...\n",
      "FAILED tests/test_nd_backend.py::test_power[cpu-shape1] - AttributeError: mod...\n",
      "FAILED tests/test_nd_backend.py::test_power[cuda-shape0] - AttributeError: mo...\n",
      "FAILED tests/test_nd_backend.py::test_power[cuda-shape1] - AttributeError: mo...\n",
      "FAILED tests/test_nd_backend.py::test_tanh[cpu-shape0] - AttributeError: modu...\n",
      "FAILED tests/test_nd_backend.py::test_tanh[cpu-shape1] - AttributeError: modu...\n",
      "FAILED tests/test_nd_backend.py::test_tanh[cuda-shape0] - AttributeError: mod...\n",
      "FAILED tests/test_nd_backend.py::test_tanh[cuda-shape1] - AttributeError: mod...\n",
      "FAILED tests/test_nd_backend.py::test_tanh_backward[cpu-shape0] - AttributeEr...\n",
      "FAILED tests/test_nd_backend.py::test_tanh_backward[cpu-shape1] - AttributeEr...\n",
      "FAILED tests/test_nd_backend.py::test_tanh_backward[cuda-shape0] - AttributeE...\n",
      "FAILED tests/test_nd_backend.py::test_tanh_backward[cuda-shape1] - AttributeE...\n",
      "FAILED tests/test_nd_backend.py::test_stack[cpu-shape0-0-1] - NotImplementedE...\n",
      "FAILED tests/test_nd_backend.py::test_stack[cpu-shape1-0-2] - NotImplementedE...\n",
      "FAILED tests/test_nd_backend.py::test_stack[cpu-shape2-2-5] - NotImplementedE...\n",
      "FAILED tests/test_nd_backend.py::test_stack[cuda-shape0-0-1] - NotImplemented...\n",
      "FAILED tests/test_nd_backend.py::test_stack[cuda-shape1-0-2] - NotImplemented...\n",
      "FAILED tests/test_nd_backend.py::test_stack[cuda-shape2-2-5] - NotImplemented...\n",
      "FAILED tests/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] - NotImp...\n",
      "FAILED tests/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] - NotImp...\n",
      "FAILED tests/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] - NotImp...\n",
      "FAILED tests/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] - NotIm...\n",
      "FAILED tests/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] - NotIm...\n",
      "FAILED tests/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] - NotIm...\n",
      "FAILED tests/test_nd_backend.py::test_summation[cpu-shape0-None] - AttributeE...\n",
      "FAILED tests/test_nd_backend.py::test_summation[cpu-shape1-0] - AttributeErro...\n",
      "FAILED tests/test_nd_backend.py::test_summation[cpu-shape2-1] - AttributeErro...\n",
      "FAILED tests/test_nd_backend.py::test_summation[cpu-shape3-2] - AttributeErro...\n",
      "FAILED tests/test_nd_backend.py::test_summation[cuda-shape0-None] - Attribute...\n",
      "FAILED tests/test_nd_backend.py::test_summation[cuda-shape1-0] - AttributeErr...\n",
      "FAILED tests/test_nd_backend.py::test_summation[cuda-shape2-1] - AttributeErr...\n",
      "FAILED tests/test_nd_backend.py::test_summation[cuda-shape3-2] - AttributeErr...\n",
      "FAILED tests/test_nd_backend.py::test_summation_backward[cpu-shape0-None] - A...\n",
      "FAILED tests/test_nd_backend.py::test_summation_backward[cpu-shape1-0] - Attr...\n",
      "FAILED tests/test_nd_backend.py::test_summation_backward[cpu-shape2-1] - Attr...\n",
      "FAILED tests/test_nd_backend.py::test_summation_backward[cpu-shape3-2] - Attr...\n",
      "FAILED tests/test_nd_backend.py::test_summation_backward[cuda-shape0-None] - ...\n",
      "FAILED tests/test_nd_backend.py::test_summation_backward[cuda-shape1-0] - Att...\n",
      "FAILED tests/test_nd_backend.py::test_summation_backward[cuda-shape2-1] - Att...\n",
      "FAILED tests/test_nd_backend.py::test_summation_backward[cuda-shape3-2] - Att...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cpu-axes0-shape0] - Attribute...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cpu-axes0-shape1] - Attribute...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cpu-axes1-shape0] - Attribute...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cpu-axes1-shape1] - Attribute...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cpu-None-shape0] - AttributeE...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cpu-None-shape1] - AttributeE...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cuda-axes0-shape0] - Attribut...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cuda-axes0-shape1] - Attribut...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cuda-axes1-shape0] - Attribut...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cuda-axes1-shape1] - Attribut...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cuda-None-shape0] - Attribute...\n",
      "FAILED tests/test_nd_backend.py::test_transpose[cuda-None-shape1] - Attribute...\n",
      "FAILED tests/test_nd_backend.py::test_logsumexp[cpu-shape0-None] - AttributeE...\n",
      "FAILED tests/test_nd_backend.py::test_logsumexp[cpu-shape1-0] - AttributeErro...\n",
      "FAILED tests/test_nd_backend.py::test_logsumexp[cpu-shape2-1] - TypeError: 'i...\n",
      "FAILED tests/test_nd_backend.py::test_logsumexp[cpu-shape3-2] - TypeError: 'i...\n",
      "FAILED tests/test_nd_backend.py::test_logsumexp[cuda-shape0-None] - Attribute...\n",
      "FAILED tests/test_nd_backend.py::test_logsumexp[cuda-shape1-0] - AttributeErr...\n",
      "FAILED tests/test_nd_backend.py::test_logsumexp[cuda-shape2-1] - TypeError: '...\n",
      "FAILED tests/test_nd_backend.py::test_logsumexp[cuda-shape3-2] - TypeError: '...\n",
      "\u001b[31m================ \u001b[31m\u001b[1m86 failed\u001b[0m, \u001b[32m32 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[31m in 6.63s\u001b[0m\u001b[31m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"nd_backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_nd_backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CIFAR-10 dataset [10 points]\n",
    "\n",
    "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images. \n",
    "\n",
    "Start by implementing the `__init__` function in the `CIFAR10Dataset` class. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Convolutional neural network [40 points]\n",
    "\n",
    "Here's an outline of what you will do in this task.\n",
    "\n",
    "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
    "- `flip`\n",
    "- `pad`\n",
    "\n",
    "In `python/needle/ops.py`, implement (forward and backward):\n",
    "- `Flip`\n",
    "- `Dilate`\n",
    "- `UnDilate`\n",
    "- `Conv`\n",
    "\n",
    "In `python/needle/nn.py`, implement:\n",
    "- `Flatten`\n",
    "- `Conv`\n",
    "\n",
    "In `python/apps/models.py`, fill in the `ResNet9` class.  \n",
    "\n",
    "In `apps/simple_training.py`, fill in:\n",
    "- `epoch_general_cifar10`,\n",
    "- `train_cifar10`\n",
    "- `evaluate_cifar10`\n",
    "\n",
    "We have provided a `BatchNorm2d` implementation for you as a wrapper around your previous `BatchNorm1d` implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ndarrays\n",
    "\n",
    "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
    "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
    "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3). \n",
    "\n",
    "Padding is also required for the backward pass of convolution.\n",
    "\n",
    "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
    "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
    "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
    "\n",
    "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"pad_forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping ndarrays & FlipOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
    "# i.e., ignoring strides\n",
    "def raw_data(X):\n",
    "    X = np.array(X) # copy, thus compact X\n",
    "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
    "\n",
    "# Xold and Xnew should reference the same underlying data\n",
    "def offset(Xold, Xnew):\n",
    "    assert Xold.itemsize == Xnew.itemsize\n",
    "    # compare addresses to the beginning of the arrays\n",
    "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
    "\n",
    "def strides(X):\n",
    "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
    "\n",
    "def format_array(X, shape):\n",
    "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i+n] for i in range(0, len(l), n))\n",
    "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
    "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
    "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
    "    return '  '.join(a)\n",
    "\n",
    "def inspect_array(X, *, is_a_copy_of):\n",
    "    # compacts X, then reads it off in order\n",
    "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
    "    # compares address of X to copy_of, thus finding X's offset\n",
    "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
    "    print('Strides: %s' % strides(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
    "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
    "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
    "\n",
    "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
    "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
    "\n",
    "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
    "\n",
    "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
    "\n",
    "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this array as reference for the other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(1, 25).reshape(3, 2, 4)\n",
    "inspect_array(A, is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you flip the array along the last axis below. \n",
    "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
    "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
    "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
    "\n",
    "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
    "to copy this behavior in our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to infer the more general algorithm for computing the offset given the axis to flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens when we flip _all_ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1,2)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flip just axes 1 and 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset is 20. Looking back on our previous offset computations, do you notice something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
    "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops.py`; note that these should be extremely short.\n",
    "\n",
    "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
    "\n",
    "Also, if you want to instead add a `flip` operator on the CPU/CUDA backends, that's also okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"flip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
    "\n",
    "\n",
    "Implement `Dilate` in `ops.py`. This function takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"dilate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit new ops (flip/dilation) to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution forward\n",
    "\n",
    "Implement the forward pass of 2D multi-channel convolution in `ops.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
    "\n",
    "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
    "\n",
    "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2). \n",
    "\n",
    "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
    "\n",
    "We recommend implementing convolution without stride first, ensuring you pass some of the tests below, and then adding in stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
    "\n",
    "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
    "\n",
    "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
    "\n",
    "`X.grad = out_grad @ W.transpose` \\\n",
    "`W.grad = X.transpose @ out_grad`\n",
    "\n",
    "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
    "\n",
    "`X.grad = conv(out_grad, W)` \\\n",
    "`W.grad = conv(X, out_grad)`\n",
    "\n",
    "In which the \"\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
    "\n",
    "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
    "\n",
    "Summarizing some hints for both `X.grad` and `W.grad`:\n",
    "\n",
    "`X.grad`\n",
    "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
    "- `W` should be flipped over both the kernel dimensions\n",
    "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
    "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape \n",
    "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
    "\n",
    "`W.grad`\n",
    "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
    "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
    "    - Consider turning batches into channels via transpose/permute\n",
    "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
    "    - Remember to account for the `padding` argument passed to convolution\n",
    "\n",
    "General tips\n",
    "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
    "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
    "- You can \"permute\" axes with multiple calls to `transpose`\n",
    "\n",
    "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fixing init._calculate_fans for convolution\n",
    "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
    "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
    "\n",
    "**You will need to edit your `kaiming_uniform`, etc. init functions to support multidimensional arrays.** In particular, you should add a new `shape` argument which is then passed to, e.g., the underlying `rand` function.\n",
    "\n",
    "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing nn.Conv\n",
    "\n",
    "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
    "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways). \n",
    "\n",
    "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
    "\n",
    "- Ensure nn.Conv works for (N, C, H, W) tensors even though we implemented the conv op for (N, H, W, C) tensors\n",
    "- Initialize the (k, k, i, o) weight tensor using Kaiming uniform initialization with default settings\n",
    "- Initialize the (o,) bias tensor using uniform initialization on the interval $\\pm$`1.0/(in_channels * kernel_size**2)**0.5`\n",
    "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
    "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
    "\n",
    "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit nn.Conv to mugrade [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing \"ResNet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
    "\n",
    "In the figure below, before the linear layer, you should \"flatten\" the tensor. We have added a module called `Flatten` in `nn.py` that you can complete and use, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
    "\n",
    "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
    "\n",
    "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
    "\n",
    "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit ResNet9 to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from models import ResNet9\n",
    "from simple_training import train_cifar10, evaluate_cifar10\n",
    "\n",
    "device = ndl.cpu()\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "dataloader = ndl.data.DataLoader(\\\n",
    "         dataset=dataset,\n",
    "         batch_size=128,\n",
    "         shuffle=True,\n",
    "         collate_fn=ndl.data.collate_ndarray,\n",
    "         device=device,\n",
    "         dtype=\"float32\")\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
    "      lr=0.001, weight_decay=0.001)\n",
    "evaluate_cifar10(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Recurrent neural network [10 points]\n",
    "\n",
    "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNNCell`.\n",
    "\n",
    "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNN`.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Long short-term memory network [10 points]\n",
    "Implement - `Sigmoid`\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$\n",
    "\n",
    "In `python/needle/nn.py`, implement `Sigmoid`, `LSTMCell` and `LSTM`.\n",
    "\n",
    "\\begin{align}\n",
    "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
    "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
    "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
    "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
    "c^\\prime &= f * c + i * g \\\\\n",
    "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively. \n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "Now implement `LSTM` in `python/needle/nn.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{align}\n",
    "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
    "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
    "c_t &= f * c_{(t-1)} + i * g \\\\\n",
    "h_t &= o * \\text{tanh}(c_t)\n",
    "\\end{align},\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively. \n",
    "\n",
    "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Penn Treebank dataset [10 points]\n",
    "\n",
    "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
    "\n",
    "In `python/needle/data.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
    "\n",
    "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
    "\n",
    "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "\n",
    "```\n",
    " a g m s \n",
    " b h n t \n",
    " c i o u \n",
    " d j p v \n",
    " e k q w \n",
    " f l r x \n",
    "```\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
    "\n",
    "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "```\n",
    " a g m s   b h n t \n",
    " b h n t   c i o u \n",
    "```\n",
    "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"ptb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ptb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training a word-level language model [10 points]\n",
    "\n",
    "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
    "\n",
    "First, in `python/needle/nn.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
    "\n",
    "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of \n",
    "\n",
    "- An embedding layer (which maps word IDs to embeddings) \n",
    "- A sequence model (either RNN or LSTM)\n",
    "- A linear layer (which outputs probabilities of the next word)\n",
    "\n",
    "In `apps/simple_training.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"language_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your language model on the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_training import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
    "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
    "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
    "evaluate_ptb(model, train_data, seq_len=40, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
